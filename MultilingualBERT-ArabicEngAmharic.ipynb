{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual BERT Approach\n",
    "\n",
    "Train on English and then Arabic and then Amharic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU presence\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT\n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT\n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT\n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT\n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF\n",
       "...      ...                                                ...       ...\n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF\n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT\n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT\n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF\n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_data = pd.read_csv('data/amharic/amharic.csv')\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT   \n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT   \n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT   \n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT   \n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF   \n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT   \n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT   \n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF   \n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF   \n",
       "\n",
       "       label  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "29995    1.0  \n",
       "29996    0.0  \n",
       "29997    0.0  \n",
       "29998    1.0  \n",
       "29999    1.0  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new column with 0/1\n",
    "#df.loc[(df[\"Q35\"]==\"N/A (no satisfaction surveys conducted)\"),\"Q37_F\"]=\n",
    "amharic_data[\"label\"] = np.nan\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "0           UNT       NaN  \n",
       "1           TIN       IND  \n",
       "2           NaN       NaN  \n",
       "3           UNT       NaN  \n",
       "4           NaN       NaN  \n",
       "...         ...       ...  \n",
       "13235       TIN       IND  \n",
       "13236       NaN       NaN  \n",
       "13237       TIN       OTH  \n",
       "13238       UNT       NaN  \n",
       "13239       NaN       NaN  \n",
       "\n",
       "[13240 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = pd.read_csv(\"data/olid/olid-training-v1.0.tsv\", sep =\"\\t\")\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  label  \n",
       "0           UNT       NaN    1.0  \n",
       "1           TIN       IND    1.0  \n",
       "2           NaN       NaN    0.0  \n",
       "3           UNT       NaN    1.0  \n",
       "4           NaN       NaN    0.0  \n",
       "...         ...       ...    ...  \n",
       "13235       TIN       IND    1.0  \n",
       "13236       NaN       NaN    0.0  \n",
       "13237       TIN       OTH    1.0  \n",
       "13238       UNT       NaN    1.0  \n",
       "13239       NaN       NaN    0.0  \n",
       "\n",
       "[13240 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english[\"label\"] = np.nan\n",
    "english.loc[(english[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "english.loc[(english[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834</th>\n",
       "      <td>7996</td>\n",
       "      <td>RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟&lt;...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>7997</td>\n",
       "      <td>RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>7998</td>\n",
       "      <td>هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>7999</td>\n",
       "      <td>RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>8000</td>\n",
       "      <td>يا سلااااام يا يو خالد انت والطرب الاصيل URL</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7839 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet subtask_a\n",
       "0        1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...       NOT\n",
       "1        2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه       NOT\n",
       "2        3  RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...       OFF\n",
       "3        4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...       NOT\n",
       "4        5          يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼       NOT\n",
       "...    ...                                                ...       ...\n",
       "7834  7996  RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟<...       NOT\n",
       "7835  7997       RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL       NOT\n",
       "7836  7998  هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...       NOT\n",
       "7837  7999  RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...       NOT\n",
       "7838  8000       يا سلااااام يا يو خالد انت والطرب الاصيل URL       NOT\n",
       "\n",
       "[7839 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic = pd.read_csv(\"data/Arabic/train.tsv\", sep = \"\\t\")\n",
    "arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>فدوه يا بخت فدوه يا زمن واحد منكم يجيبه</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7834</th>\n",
       "      <td>7996</td>\n",
       "      <td>RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟&lt;...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7835</th>\n",
       "      <td>7997</td>\n",
       "      <td>RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7836</th>\n",
       "      <td>7998</td>\n",
       "      <td>هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7837</th>\n",
       "      <td>7999</td>\n",
       "      <td>RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7838</th>\n",
       "      <td>8000</td>\n",
       "      <td>يا سلااااام يا يو خالد انت والطرب الاصيل URL</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7839 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet subtask_a  label\n",
       "0        1  الحمدلله يارب فوز مهم يا زمالك.. كل الدعم ليكم...       NOT    0.0\n",
       "1        2            فدوه يا بخت فدوه يا زمن واحد منكم يجيبه       NOT    0.0\n",
       "2        3  RT @USER: يا رب يا واحد يا أحد بحق يوم الاحد ا...       OFF    1.0\n",
       "3        4  RT @USER: #هوا_الحرية يا وجع قلبي عليكي يا امي...       NOT    0.0\n",
       "4        5          يا بكون بحياتك الأهم يا إما ما بدي أكون 🎼       NOT    0.0\n",
       "...    ...                                                ...       ...    ...\n",
       "7834  7996  RT @USER: انتو بتوزعوا زيت وسكر فعلا يا عباس؟<...       NOT    0.0\n",
       "7835  7997       RT @USER: كدا يا عمر متزعلهاش يا حبيبي 😂 URL       NOT    0.0\n",
       "7836  7998  هدا سكن اطفال امارتين من شارقة طالبين فزعتكم ي...       NOT    0.0\n",
       "7837  7999  RT @USER: ومدني بمدد من قوتك أواجه به ضعفي.. و...       NOT    0.0\n",
       "7838  8000       يا سلااااام يا يو خالد انت والطرب الاصيل URL       NOT    0.0\n",
       "\n",
       "[7839 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic[\"label\"] = np.nan\n",
    "arabic.loc[(arabic[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "arabic.loc[(arabic[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 4)\n",
      "(1500, 4)\n",
      "(1500, 4)\n"
     ]
    }
   ],
   "source": [
    "#load our tuned Amharic dataset\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.9)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)\n",
    "print(amharic_train.shape)\n",
    "print(amharic_test.shape)\n",
    "print(amharic_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>20950</td>\n",
       "      <td>አትቀባጥር ማፊያዎች መጥፊያቹ እየደረሰ ነውየትግራይን ህዝብ መጡብህ እያል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28510</th>\n",
       "      <td>28510</td>\n",
       "      <td>ስለ ሙስሊም መብት ሙስሊም ይጠይቅ አንተ ሙስሊምን ወክለህ መጠየቅ አትችልም</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15669</th>\n",
       "      <td>15669</td>\n",
       "      <td>ለሱ የተመኘሀው ሞተ ለጠላቱ ክፉ ለሚመኝለት ይሁን</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15875</th>\n",
       "      <td>15875</td>\n",
       "      <td>ሚኪ በመምጣትህ ደስ ብሎናል</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>13655</td>\n",
       "      <td>ደደብ</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7776</th>\n",
       "      <td>7776</td>\n",
       "      <td>ጌጡ የሚረባ ነገር ፃፍ አታበሳጨኝ በአንድ ጊዜ የድራማ ደረሲ አደረገህ ፌ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>573</td>\n",
       "      <td>አምላክ ሆይ ለኢትዮጵያ የገባህላትን ቃል አስታውስ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>646</td>\n",
       "      <td>ያማል በጣም ተዋዶ ተጋብቶ ተዋልዶ የኖረ ያ ፍቅር የሆነ ህዝብ ሲገዳደልአ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19360</th>\n",
       "      <td>19360</td>\n",
       "      <td>እዚህ ጋር የተጋሩስ መንጫጫት የነገረኝ መገንጠልን ያወሩታል እንጂ ለመፈፀ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>2990</td>\n",
       "      <td>ቋንቋ ሆድ አይሞላ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "20950  20950  አትቀባጥር ማፊያዎች መጥፊያቹ እየደረሰ ነውየትግራይን ህዝብ መጡብህ እያል...       NOT   \n",
       "28510  28510    ስለ ሙስሊም መብት ሙስሊም ይጠይቅ አንተ ሙስሊምን ወክለህ መጠየቅ አትችልም       NOT   \n",
       "15669  15669                    ለሱ የተመኘሀው ሞተ ለጠላቱ ክፉ ለሚመኝለት ይሁን       OFF   \n",
       "15875  15875                                  ሚኪ በመምጣትህ ደስ ብሎናል       NOT   \n",
       "13655  13655                                                ደደብ       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "7776    7776  ጌጡ የሚረባ ነገር ፃፍ አታበሳጨኝ በአንድ ጊዜ የድራማ ደረሲ አደረገህ ፌ...       OFF   \n",
       "573      573                    አምላክ ሆይ ለኢትዮጵያ የገባህላትን ቃል አስታውስ       NOT   \n",
       "646      646  ያማል በጣም ተዋዶ ተጋብቶ ተዋልዶ የኖረ ያ ፍቅር የሆነ ህዝብ ሲገዳደልአ...       NOT   \n",
       "19360  19360  እዚህ ጋር የተጋሩስ መንጫጫት የነገረኝ መገንጠልን ያወሩታል እንጂ ለመፈፀ...       OFF   \n",
       "2990    2990                                        ቋንቋ ሆድ አይሞላ       NOT   \n",
       "\n",
       "       label  \n",
       "20950    0.0  \n",
       "28510    0.0  \n",
       "15669    1.0  \n",
       "15875    0.0  \n",
       "13655    1.0  \n",
       "...      ...  \n",
       "7776     1.0  \n",
       "573      0.0  \n",
       "646      0.0  \n",
       "19360    1.0  \n",
       "2990     0.0  \n",
       "\n",
       "[27000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length tweet is: 254\n",
      "min length tweet is: 1\n",
      "mean length tweet is: 18.026133333333334\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum length tweet\n",
    "print(\"max length tweet is:\",np.max([len(x.split()) for x in amharic_data.tweet]))\n",
    "print(\"min length tweet is:\",np.min([len(x.split()) for x in amharic_data.tweet]))\n",
    "print(\"mean length tweet is:\",np.mean([len(x.split()) for x in amharic_data.tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#NEED to import and load both of these\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "=================================================================\n",
      "Total params: 167,356,416\n",
      "Trainable params: 167,356,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 100 #use to to a bit larger than the mean tweet length\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = amharic_train.label\n",
    "\n",
    "eng_x_train = tokenizer([x for x in english.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "eng_y_train = english.label\n",
    "\n",
    "\n",
    "arabic_x_train = tokenizer([x for x in arabic.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "arabic_y_train = arabic.label\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = amharic_dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of positive examples:  0.5070370370370371\n"
     ]
    }
   ],
   "source": [
    "#Let's look at class imbalance\n",
    "print('ratio of positive examples: ', np.sum(y_train==1)/len(y_train))\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From BERT_Fine_tuning Walkthrough Notebook/Session\n",
    "\n",
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs) #same as x_tiny example above, always set ouput to model acting on input\n",
    "\n",
    "    \n",
    "    #getting the CLS token, could change to bert_out[1]\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0]) \n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Creating models and changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Fit on English and then Amharic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1655/1655 [==============================] - 416s 246ms/step - loss: 0.7660 - accuracy: 0.6241 - val_loss: 0.6358 - val_accuracy: 0.6677\n",
      "Epoch 2/5\n",
      "1655/1655 [==============================] - 405s 245ms/step - loss: 0.6462 - accuracy: 0.6622 - val_loss: 0.6360 - val_accuracy: 0.6677\n",
      "Epoch 3/5\n",
      "1655/1655 [==============================] - 406s 245ms/step - loss: 0.6406 - accuracy: 0.6658 - val_loss: 0.6389 - val_accuracy: 0.6677\n",
      "Epoch 4/5\n",
      "1655/1655 [==============================] - 405s 245ms/step - loss: 0.6434 - accuracy: 0.6605 - val_loss: 0.6387 - val_accuracy: 0.6677\n",
      "Epoch 5/5\n",
      "1655/1655 [==============================] - 405s 245ms/step - loss: 0.6390 - accuracy: 0.6686 - val_loss: 0.6369 - val_accuracy: 0.6677\n",
      "Epoch 1/5\n",
      "3375/3375 [==============================] - 674s 200ms/step - loss: 0.6951 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5067\n",
      "Epoch 2/5\n",
      "3375/3375 [==============================] - 671s 199ms/step - loss: 0.6943 - accuracy: 0.5022 - val_loss: 0.6941 - val_accuracy: 0.4933\n",
      "Epoch 3/5\n",
      "3375/3375 [==============================] - 671s 199ms/step - loss: 0.6939 - accuracy: 0.5041 - val_loss: 0.6943 - val_accuracy: 0.4933\n",
      "Epoch 4/5\n",
      "3375/3375 [==============================] - 670s 199ms/step - loss: 0.6942 - accuracy: 0.5045 - val_loss: 0.6945 - val_accuracy: 0.5067\n",
      "Epoch 5/5\n",
      "3375/3375 [==============================] - 670s 199ms/step - loss: 0.6942 - accuracy: 0.5002 - val_loss: 0.6945 - val_accuracy: 0.4933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2bbc3b1700>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit on English\n",
    "classification_model.fit([eng_x_train.input_ids, eng_x_train.token_type_ids, eng_x_train.attention_mask],\n",
    "                         eng_y_train,\n",
    "                         validation_data=([eng_x_train.input_ids, eng_x_train.token_type_ids, eng_x_train.attention_mask],\n",
    "                         eng_y_train),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)\n",
    "\n",
    "\n",
    "#Fit on Amharic train\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27000, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask]) \n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 167356416   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 167,510,417\n",
      "Trainable params: 167,510,417\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([27000, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6932680010795593 / Train accuracy: 0.5084444284439087\n"
     ]
    }
   ],
   "source": [
    "score = classification_model.evaluate([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6944733262062073 / Test accuracy: 0.4933333396911621\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Following Model 2 from BERT Walkthrough notebook\n",
    "Updating learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3375/3375 [==============================] - 682s 200ms/step - loss: 0.7138 - accuracy: 0.5033 - val_loss: 0.7106 - val_accuracy: 0.4933\n",
      "Epoch 2/5\n",
      "3375/3375 [==============================] - 672s 199ms/step - loss: 0.6993 - accuracy: 0.5062 - val_loss: 0.6971 - val_accuracy: 0.4933\n",
      "Epoch 3/5\n",
      "3375/3375 [==============================] - 672s 199ms/step - loss: 0.6984 - accuracy: 0.4970 - val_loss: 0.6936 - val_accuracy: 0.4933\n",
      "Epoch 4/5\n",
      "3375/3375 [==============================] - 672s 199ms/step - loss: 0.6976 - accuracy: 0.4979 - val_loss: 0.6955 - val_accuracy: 0.4933\n",
      "Epoch 5/5\n",
      "3375/3375 [==============================] - 676s 200ms/step - loss: 0.6953 - accuracy: 0.5050 - val_loss: 0.6940 - val_accuracy: 0.5067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.47117275],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.47117275],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ],\n",
       "       [0.4711727 ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same thing as above but change learning rate in Adam below, need to get fresh bert model\n",
    "try:\n",
    "    del classification_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del bert_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))\n",
    "\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)\n",
    "\n",
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=8, \n",
    "                             steps=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6957873106002808 / Train accuracy: 0.4915555417537689\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train, verbose=0)\n",
    "print(f'Train loss: {score[0]} / Train accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6977327466011047 / Test accuracy: 0.4959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 108,464,273\n",
      "Trainable params: 108,464,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b12a87bde52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/BERT_multilingual_adam_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/BERT_multilingual_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "classification_model.save_model(\"models/BERT_multilingual_adam_v1\")\n",
    "tokenizer.save_pretrained(\"tokenizers/BERT_multilingual_v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
