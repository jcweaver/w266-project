{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual BERT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU presence\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT\n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT\n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT\n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT\n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF\n",
       "...      ...                                                ...       ...\n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF\n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT\n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT\n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF\n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_data = pd.read_csv('data/amharic/amharic.csv')\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT   \n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT   \n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT   \n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT   \n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF   \n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT   \n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT   \n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF   \n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF   \n",
       "\n",
       "       label  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "29995    1.0  \n",
       "29996    0.0  \n",
       "29997    0.0  \n",
       "29998    1.0  \n",
       "29999    1.0  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new column with 0/1\n",
    "#df.loc[(df[\"Q35\"]==\"N/A (no satisfaction surveys conducted)\"),\"Q37_F\"]=\n",
    "amharic_data[\"label\"] = np.nan\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 4)\n",
      "(1500, 4)\n",
      "(1500, 4)\n"
     ]
    }
   ],
   "source": [
    "#load our tuned Amharic dataset\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.9)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)\n",
    "print(amharic_train.shape)\n",
    "print(amharic_test.shape)\n",
    "print(amharic_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>5137</td>\n",
       "      <td>ትገረማለህ አንተ ግን ጠምንሰተሪ ሰትባል ፐሪኮንሽዮሰ የለህም ጣውላ ራስ</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28676</th>\n",
       "      <td>28676</td>\n",
       "      <td>ሚኪ እናመስግናለን እውነት_ትደበቃለች_እንጂ_አትጠፋም ስልጣኔውም ሆነ ፊደ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18291</th>\n",
       "      <td>18291</td>\n",
       "      <td>መጀመሪያም የተናገርኩት ይህን ነው መብራት እና ውሀ በፈረቃ ባለበት ሀገር...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4824</th>\n",
       "      <td>4824</td>\n",
       "      <td>አየ ደክተር አብይ እዚህ ህዝብ እያለቀ አንተ ሽርጉድ ትላለህ ማለቃችን ተ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>7249</td>\n",
       "      <td>ሰው ሁሉ በእንተርኔት ሆነ እንዴ ፀሌቱ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10111</th>\n",
       "      <td>10111</td>\n",
       "      <td>የወያኔ ጊዜውን የዘይት እጥረት ለመሸፈን የአለም ሀገራት የሸሹትንና ያገዱ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>2577</td>\n",
       "      <td>ዶር አለሙ ስሜ የሜቴክ ዋና ዳሬክተር</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6264</th>\n",
       "      <td>6264</td>\n",
       "      <td>እውነትን እውነት ሀሰትን ሀሰት በል ሀብታሙ አያሌው በእውነት ይለያልደስ ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27175</th>\n",
       "      <td>27175</td>\n",
       "      <td>ጃዋር በእናትህ ሜዳ ላይ ከታደለ ጋ ተጋጠሙ ያኔ ወንድ ጀግና እልሀለሁ ካ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17728</th>\n",
       "      <td>17728</td>\n",
       "      <td>ስለ ጎደኞቹ ሀሜት ይነግርህ ነበረ ስለምናላቸው እና ሚስቱ ደግሞ አብዝቶ ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "5137    5137      ትገረማለህ አንተ ግን ጠምንሰተሪ ሰትባል ፐሪኮንሽዮሰ የለህም ጣውላ ራስ       OFF   \n",
       "28676  28676  ሚኪ እናመስግናለን እውነት_ትደበቃለች_እንጂ_አትጠፋም ስልጣኔውም ሆነ ፊደ...       NOT   \n",
       "18291  18291  መጀመሪያም የተናገርኩት ይህን ነው መብራት እና ውሀ በፈረቃ ባለበት ሀገር...       OFF   \n",
       "4824    4824  አየ ደክተር አብይ እዚህ ህዝብ እያለቀ አንተ ሽርጉድ ትላለህ ማለቃችን ተ...       OFF   \n",
       "7249    7249                           ሰው ሁሉ በእንተርኔት ሆነ እንዴ ፀሌቱ       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "10111  10111  የወያኔ ጊዜውን የዘይት እጥረት ለመሸፈን የአለም ሀገራት የሸሹትንና ያገዱ...       NOT   \n",
       "2577    2577                            ዶር አለሙ ስሜ የሜቴክ ዋና ዳሬክተር       NOT   \n",
       "6264    6264  እውነትን እውነት ሀሰትን ሀሰት በል ሀብታሙ አያሌው በእውነት ይለያልደስ ...       NOT   \n",
       "27175  27175  ጃዋር በእናትህ ሜዳ ላይ ከታደለ ጋ ተጋጠሙ ያኔ ወንድ ጀግና እልሀለሁ ካ...       OFF   \n",
       "17728  17728  ስለ ጎደኞቹ ሀሜት ይነግርህ ነበረ ስለምናላቸው እና ሚስቱ ደግሞ አብዝቶ ...       OFF   \n",
       "\n",
       "       label  \n",
       "5137     1.0  \n",
       "28676    0.0  \n",
       "18291    1.0  \n",
       "4824     1.0  \n",
       "7249     0.0  \n",
       "...      ...  \n",
       "10111    0.0  \n",
       "2577     0.0  \n",
       "6264     0.0  \n",
       "27175    1.0  \n",
       "17728    1.0  \n",
       "\n",
       "[27000 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length tweet is: 254\n",
      "min length tweet is: 1\n",
      "mean length tweet is: 18.026133333333334\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum length tweet\n",
    "print(\"max length tweet is:\",np.max([len(x.split()) for x in amharic_data.tweet]))\n",
    "print(\"min length tweet is:\",np.min([len(x.split()) for x in amharic_data.tweet]))\n",
    "print(\"mean length tweet is:\",np.mean([len(x.split()) for x in amharic_data.tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "=================================================================\n",
      "Total params: 167,356,416\n",
      "Trainable params: 167,356,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 100 #use to to a bit larger than the mean tweet length\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = amharic_train.label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = amharic_dev.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of positive examples:  0.5066666666666667\n"
     ]
    }
   ],
   "source": [
    "#Let's look at class imbalance\n",
    "print('ratio of positive examples: ', np.sum(y_train==1)/len(y_train))\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(27000, 100), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From BERT_Fine_tuning Walkthrough Notebook/Session\n",
    "\n",
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs) #same as x_tiny example above, always set ouput to model acting on input\n",
    "\n",
    "    \n",
    "    #getting the CLS token, could change to bert_out[1]\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0]) \n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Creating models and changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Following Model 1 from BERT_Fine_tuning walkthrough notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2700/2700 [==============================] - 649s 238ms/step - loss: 0.6970 - accuracy: 0.5031 - val_loss: 0.6994 - val_accuracy: 0.4987\n",
      "Epoch 2/3\n",
      "2700/2700 [==============================] - 642s 238ms/step - loss: 0.6940 - accuracy: 0.5014 - val_loss: 0.6932 - val_accuracy: 0.4987\n",
      "Epoch 3/3\n",
      "2700/2700 [==============================] - 642s 238ms/step - loss: 0.6941 - accuracy: 0.5049 - val_loss: 0.6933 - val_accuracy: 0.4987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f798c2155b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This took a long time, may want to increase batch_size for next run?\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=3,\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5082261],\n",
       "       [0.5082261],\n",
       "       [0.5082261],\n",
       "       ...,\n",
       "       [0.5082261],\n",
       "       [0.5082261],\n",
       "       [0.5082261]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], ) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6933264136314392 / Test accuracy: 0.4986666738986969\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7565c2f4f57a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_label\"] = np.nan\n",
      "<ipython-input-22-7565c2f4f57a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_stat\"] = predictions\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "#Creating some new columns & printing out a csv with the predicted labels\n",
    "amharic_train[\"predicted_label\"] = np.nan\n",
    "amharic_train[\"predicted_stat\"] = predictions\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] >= 0.5), \"predicted_label\"] = 1\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] < 0.5), \"predicted_label\"] = 0\n",
    "amharic_train.to_csv(\"Amharic_train_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Following Model 2 from BERT Walkthrough notebook\n",
    "Updating learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3375/3375 [==============================] - 704s 206ms/step - loss: 0.7039 - accuracy: 0.5055 - val_loss: 0.6936 - val_accuracy: 0.4960\n",
      "Epoch 2/5\n",
      "3375/3375 [==============================] - 692s 205ms/step - loss: 0.7004 - accuracy: 0.4996 - val_loss: 0.6933 - val_accuracy: 0.4960\n",
      "Epoch 3/5\n",
      "3375/3375 [==============================] - 691s 205ms/step - loss: 0.6967 - accuracy: 0.5042 - val_loss: 0.6961 - val_accuracy: 0.4960\n",
      "Epoch 4/5\n",
      "3375/3375 [==============================] - 691s 205ms/step - loss: 0.6972 - accuracy: 0.4988 - val_loss: 0.6959 - val_accuracy: 0.5040\n",
      "Epoch 5/5\n",
      "3375/3375 [==============================] - 691s 205ms/step - loss: 0.6971 - accuracy: 0.4970 - val_loss: 0.6977 - val_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439621],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622],\n",
       "       [0.5439622]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same thing as above but change learning rate in Adam below, need to get fresh bert model\n",
    "try:\n",
    "    del classification_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del bert_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))\n",
    "\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)\n",
    "\n",
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=8, \n",
    "                             steps=2)\n",
    "\n",
    "\n",
    "#This looks a little worse, not sure why it's now predicting 54% consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6977327466011047 / Test accuracy: 0.4959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 108,464,273\n",
      "Trainable params: 108,464,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b12a87bde52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/BERT_multilingual_adam_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/BERT_multilingual_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "classification_model.save_model(\"models/BERT_multilingual_adam_v1\")\n",
    "tokenizer.save_pretrained(\"tokenizers/BERT_multilingual_v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
