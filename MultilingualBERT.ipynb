{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual BERT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU presence\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT\n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT\n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT\n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT\n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF\n",
       "...      ...                                                ...       ...\n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF\n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT\n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT\n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF\n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_data = pd.read_csv('data/amharic/amharic.csv')\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT   \n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT   \n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT   \n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT   \n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF   \n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT   \n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT   \n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF   \n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF   \n",
       "\n",
       "       label  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "29995    1.0  \n",
       "29996    0.0  \n",
       "29997    0.0  \n",
       "29998    1.0  \n",
       "29999    1.0  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new column with 0/1\n",
    "#df.loc[(df[\"Q35\"]==\"N/A (no satisfaction surveys conducted)\"),\"Q37_F\"]=\n",
    "amharic_data[\"label\"] = np.nan\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21000, 4)\n",
      "(4500, 4)\n",
      "(4500, 4)\n"
     ]
    }
   ],
   "source": [
    "#load our tuned Amharic dataset\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.7)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)\n",
    "print(amharic_train.shape)\n",
    "print(amharic_test.shape)\n",
    "print(amharic_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8761</th>\n",
       "      <td>8761</td>\n",
       "      <td>ሀሳብህ ታላቅ ሩቅ ተመልካአሳመነው ህዝቡን ወዳጅ ነፍስህን ሰጠኽን በአዋጅ</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16929</th>\n",
       "      <td>16929</td>\n",
       "      <td>በርቱ  ማሻ አሏህ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21761</th>\n",
       "      <td>21761</td>\n",
       "      <td>ትክክል የገዳ ስርአተ ትምህርት በመሰጠቱ ደስ ብሎኛልምክንያቱም የኦሮሞ ህ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5449</th>\n",
       "      <td>5449</td>\n",
       "      <td>ፋና እውነትም ብታዎሩ እናተ ውሸታም ስለሆናችሁ የሚሰማችሁ የለም ከአንበጣ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20279</th>\n",
       "      <td>20279</td>\n",
       "      <td>ሰለሞን ዘባህርዳር ተረት ተረት የመሰረት ድሮ አንድ ሰው ይኖር ነበረ   ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9642</th>\n",
       "      <td>9642</td>\n",
       "      <td>ልምን ተብሎ የወጣ ዶላር ነው እንክት አርገህ የበላሀው በአሁን ሰአት ሁሉ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6390</th>\n",
       "      <td>6390</td>\n",
       "      <td>ለማስመሰል እንኳን ንጉሱን ወይ መንጌን ወይ ዜናዊን እንዲህ ያታዩበትን ማ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5851</th>\n",
       "      <td>5851</td>\n",
       "      <td>የፖለቲካ ፓርቲዎች የጋራ ምክር ቤት በኢትዮጵያ ብሄራዊ ምርጫ ቦርድ በኩል...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19518</th>\n",
       "      <td>19518</td>\n",
       "      <td>ተራና ነገ በታሪክ ፊት የሚያሳፍር ብሽሽቅ ውስጥ አንድ ድርጅት ሲገባ ዝም...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>1502</td>\n",
       "      <td>ችግራቹን እዛው ጨርሱ ሙስሊም እንደናንተ በሌላ ሀይማኖት ተመሳስሎ ለመግባ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "8761    8761     ሀሳብህ ታላቅ ሩቅ ተመልካአሳመነው ህዝቡን ወዳጅ ነፍስህን ሰጠኽን በአዋጅ       OFF   \n",
       "16929  16929                                        በርቱ  ማሻ አሏህ       NOT   \n",
       "21761  21761  ትክክል የገዳ ስርአተ ትምህርት በመሰጠቱ ደስ ብሎኛልምክንያቱም የኦሮሞ ህ...       NOT   \n",
       "5449    5449  ፋና እውነትም ብታዎሩ እናተ ውሸታም ስለሆናችሁ የሚሰማችሁ የለም ከአንበጣ...       OFF   \n",
       "20279  20279  ሰለሞን ዘባህርዳር ተረት ተረት የመሰረት ድሮ አንድ ሰው ይኖር ነበረ   ...       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "9642    9642  ልምን ተብሎ የወጣ ዶላር ነው እንክት አርገህ የበላሀው በአሁን ሰአት ሁሉ...       OFF   \n",
       "6390    6390  ለማስመሰል እንኳን ንጉሱን ወይ መንጌን ወይ ዜናዊን እንዲህ ያታዩበትን ማ...       OFF   \n",
       "5851    5851  የፖለቲካ ፓርቲዎች የጋራ ምክር ቤት በኢትዮጵያ ብሄራዊ ምርጫ ቦርድ በኩል...       OFF   \n",
       "19518  19518  ተራና ነገ በታሪክ ፊት የሚያሳፍር ብሽሽቅ ውስጥ አንድ ድርጅት ሲገባ ዝም...       OFF   \n",
       "1502    1502  ችግራቹን እዛው ጨርሱ ሙስሊም እንደናንተ በሌላ ሀይማኖት ተመሳስሎ ለመግባ...       OFF   \n",
       "\n",
       "       label  \n",
       "8761     1.0  \n",
       "16929    0.0  \n",
       "21761    0.0  \n",
       "5449     1.0  \n",
       "20279    1.0  \n",
       "...      ...  \n",
       "9642     1.0  \n",
       "6390     1.0  \n",
       "5851     1.0  \n",
       "19518    1.0  \n",
       "1502     1.0  \n",
       "\n",
       "[21000 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length tweet is: 1326\n",
      "min length tweet is: 2\n",
      "mean length tweet is: 88.00763333333333\n"
     ]
    }
   ],
   "source": [
    "#Find the maximum length tweet\n",
    "print(\"max length tweet is:\",np.max([len(x) for x in amharic_data.tweet]))\n",
    "print(\"min length tweet is:\",np.min([len(x) for x in amharic_data.tweet]))\n",
    "print(\"mean length tweet is:\",np.mean([len(x) for x in amharic_data.tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a few dev and train examples\n",
    "\n",
    "num_train_examples = 2500\n",
    "num_dev_examples = 500\n",
    "num_tiny_set = 5\n",
    "\n",
    "max_length = 100 #use to to a bit larger than the mean tweet length\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet[:num_train_examples]], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = amharic_train.label[:num_train_examples]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet[:num_dev_examples]], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = amharic_dev.label[:num_dev_examples]\n",
    "\n",
    "\n",
    "x_tiny = tokenizer([x for x in amharic_dev.tweet[:num_tiny_set]], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_tiny = amharic_dev.label[:num_tiny_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of positive examples:  0.5124\n"
     ]
    }
   ],
   "source": [
    "#Let's look at class imbalance\n",
    "print('ratio of positive examples: ', np.sum(y_train==1)/len(y_train))\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2500, 100), dtype=int32, numpy=\n",
       "array([[101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ..., 100, 100, 102]], dtype=int32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From BERT_Fine_tuning Walkthrough Notebook/Session\n",
    "\n",
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs) #same as x_tiny example above, always set ouput to model acting on input\n",
    "\n",
    "    \n",
    "    #getting the CLS token, could change to ber_out[1]\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0]) \n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(classification_token)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Creating models and changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Following Model 1 from BERT_Fine_tuning walkthrough notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 86s 233ms/step - loss: 0.8184 - accuracy: 0.5031 - val_loss: 0.7640 - val_accuracy: 0.5160\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 70s 224ms/step - loss: 0.7320 - accuracy: 0.4773 - val_loss: 0.7332 - val_accuracy: 0.5160\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 0.7221 - accuracy: 0.4746 - val_loss: 0.7410 - val_accuracy: 0.4840\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 0.7036 - accuracy: 0.5310 - val_loss: 0.7055 - val_accuracy: 0.5160\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 70s 223ms/step - loss: 0.7083 - accuracy: 0.4921 - val_loss: 0.7064 - val_accuracy: 0.4840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1f9d5eca0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347937],\n",
       "       [0.43347943],\n",
       "       [0.43347937],\n",
       "       [0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347943],\n",
       "       [0.43347937],\n",
       "       [0.43347937],\n",
       "       [0.43347937],\n",
       "       [0.43347943],\n",
       "       [0.43347943]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=8, \n",
    "                             steps=2) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Following Model 2 from BERT Walkthrough notebook\n",
    "Updating learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "313/313 [==============================] - 78s 206ms/step - loss: 0.7880 - accuracy: 0.5312 - val_loss: 0.7052 - val_accuracy: 0.4840\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 61s 196ms/step - loss: 0.7342 - accuracy: 0.4765 - val_loss: 0.9142 - val_accuracy: 0.4840\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 61s 195ms/step - loss: 0.7302 - accuracy: 0.5037 - val_loss: 0.7013 - val_accuracy: 0.5160\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 61s 195ms/step - loss: 0.7051 - accuracy: 0.5101 - val_loss: 0.7125 - val_accuracy: 0.5160\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 61s 195ms/step - loss: 0.7017 - accuracy: 0.5054 - val_loss: 0.6930 - val_accuracy: 0.5160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.50230324],\n",
       "       [0.50211066],\n",
       "       [0.50244266],\n",
       "       [0.5025217 ],\n",
       "       [0.5025738 ],\n",
       "       [0.50223875],\n",
       "       [0.5025535 ],\n",
       "       [0.50233537],\n",
       "       [0.5023818 ],\n",
       "       [0.50226843],\n",
       "       [0.50230324],\n",
       "       [0.5026371 ],\n",
       "       [0.50241315],\n",
       "       [0.50262165],\n",
       "       [0.5023784 ],\n",
       "       [0.50214475]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same thing as above but change learning rate in Adam below, need to get fresh bert model\n",
    "try:\n",
    "    del classification_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del bert_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))\n",
    "\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=8)\n",
    "\n",
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], \n",
    "                             batch_size=8, \n",
    "                             steps=2)\n",
    "\n",
    "\n",
    "#This looks even worse, as now model is predicting even close to 50% of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6930242776870728 / Test accuracy: 0.515999972820282\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
