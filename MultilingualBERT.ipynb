{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual BERT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU presence\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT\n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT\n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT\n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT\n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF\n",
       "...      ...                                                ...       ...\n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF\n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT\n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT\n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF\n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF\n",
       "\n",
       "[30000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_data = pd.read_csv('data/amharic/amharic.csv')\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995</td>\n",
       "      <td>በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996</td>\n",
       "      <td>ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997</td>\n",
       "      <td>እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998</td>\n",
       "      <td>ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999</td>\n",
       "      <td>ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0          0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT   \n",
       "1          1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT   \n",
       "2          2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT   \n",
       "3          3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT   \n",
       "4          4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "29995  29995         በአሉ የሁሉም ኢትዮጵያዊ ስላልሆነ በኦሮምኛው ቢለፋደድ ምን አገባን       OFF   \n",
       "29996  29996             ተባረክ አብቹ ፈር ቀዳጅ ስለሆንህ መጋረጃው መቀደድ ስለጀመረ       NOT   \n",
       "29997  29997  እስከ አሁን አንተ ብቻ ነው በ   መፅሀፍ ያልቻልከው  አንተም ታሪክ እን...       NOT   \n",
       "29998  29998  ህገወጥት ጠቅላይ ሚንስትር ፅቤት የተፈቀደ ሆኖ ህዝብን እንዴት ህግ አክብ...       OFF   \n",
       "29999  29999  ደነዙ ጠቅላይ ሚንስትር ፅቤት ህገመንግስት ሳይሻሻል በህግ የተወሰነዉን የ...       OFF   \n",
       "\n",
       "       label  \n",
       "0        0.0  \n",
       "1        0.0  \n",
       "2        0.0  \n",
       "3        0.0  \n",
       "4        1.0  \n",
       "...      ...  \n",
       "29995    1.0  \n",
       "29996    0.0  \n",
       "29997    0.0  \n",
       "29998    1.0  \n",
       "29999    1.0  \n",
       "\n",
       "[30000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new column with 0/1\n",
    "amharic_data[\"label\"] = np.nan\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 4)\n",
      "(3000, 4)\n",
      "(3000, 4)\n"
     ]
    }
   ],
   "source": [
    "#load our tuned Amharic dataset\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.8)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)\n",
    "print(amharic_train.shape)\n",
    "print(amharic_test.shape)\n",
    "print(amharic_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21559</th>\n",
       "      <td>21559</td>\n",
       "      <td>የሆነው እና እየሆነ ያለውም ያው እና ተመሳሳይ ነው ይልቅ አጀንዳው ይዘጋ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15861</th>\n",
       "      <td>15861</td>\n",
       "      <td>ድክመታቸውን እንድህ እየጠቆማችሁ እኛው ላይ አበርቷቸው ጥሩ ነው¡</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19571</th>\n",
       "      <td>19571</td>\n",
       "      <td>መምህሬ በክፍል ስያስረዱ ቆይተው ቀላሏን ጥያቀ ጠይቀው ካልመለስክ አህያ ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28599</th>\n",
       "      <td>28599</td>\n",
       "      <td>የሶን ቪዲዮ በብዛት ማየት ከጀመርኩ በሀላ ፀሎት ስግደት ቀድሞ ቅዳሴ መግ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3385</th>\n",
       "      <td>3385</td>\n",
       "      <td>የአባቶቹ ልጂ</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16319</th>\n",
       "      <td>16319</td>\n",
       "      <td>አቤት ዘመዴ ያንተን ፎቶ እስክሪን ሴበር አድርጌ ስንት ሰው ጋር ተጣልቸበታለሁ</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1879</th>\n",
       "      <td>1879</td>\n",
       "      <td>የህልውና ትግላችን የሁላችንንም ሙሎ ተሳትፎ የሚጠይቅ ስለሆነ ለአፍታም ከ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28832</th>\n",
       "      <td>28832</td>\n",
       "      <td>እጅግ ተናፍቀህ ነበር</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9775</th>\n",
       "      <td>9775</td>\n",
       "      <td>አንቀፅ  ላይ አገሪቱ አፍሪካ ውስጥ መሆኗን ይገልፅና አፍሪካ ውስጥ ደግሞ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26404</th>\n",
       "      <td>26404</td>\n",
       "      <td>ወንድም ዳኒ አንተ እና የስጋ ዘመዶችህ ወያኔወች ለ አመት ባንዲራው ላይ ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "21559  21559  የሆነው እና እየሆነ ያለውም ያው እና ተመሳሳይ ነው ይልቅ አጀንዳው ይዘጋ...       NOT   \n",
       "15861  15861          ድክመታቸውን እንድህ እየጠቆማችሁ እኛው ላይ አበርቷቸው ጥሩ ነው¡       NOT   \n",
       "19571  19571  መምህሬ በክፍል ስያስረዱ ቆይተው ቀላሏን ጥያቀ ጠይቀው ካልመለስክ አህያ ...       OFF   \n",
       "28599  28599  የሶን ቪዲዮ በብዛት ማየት ከጀመርኩ በሀላ ፀሎት ስግደት ቀድሞ ቅዳሴ መግ...       NOT   \n",
       "3385    3385                                           የአባቶቹ ልጂ       OFF   \n",
       "...      ...                                                ...       ...   \n",
       "16319  16319  አቤት ዘመዴ ያንተን ፎቶ እስክሪን ሴበር አድርጌ ስንት ሰው ጋር ተጣልቸበታለሁ       NOT   \n",
       "1879    1879  የህልውና ትግላችን የሁላችንንም ሙሎ ተሳትፎ የሚጠይቅ ስለሆነ ለአፍታም ከ...       NOT   \n",
       "28832  28832                                      እጅግ ተናፍቀህ ነበር       NOT   \n",
       "9775    9775  አንቀፅ  ላይ አገሪቱ አፍሪካ ውስጥ መሆኗን ይገልፅና አፍሪካ ውስጥ ደግሞ...       OFF   \n",
       "26404  26404  ወንድም ዳኒ አንተ እና የስጋ ዘመዶችህ ወያኔወች ለ አመት ባንዲራው ላይ ...       OFF   \n",
       "\n",
       "       label  \n",
       "21559    0.0  \n",
       "15861    0.0  \n",
       "19571    1.0  \n",
       "28599    0.0  \n",
       "3385     1.0  \n",
       "...      ...  \n",
       "16319    0.0  \n",
       "1879     0.0  \n",
       "28832    0.0  \n",
       "9775     1.0  \n",
       "26404    1.0  \n",
       "\n",
       "[24000 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in amharic_train.tweet]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "\n",
    "#It looks like the max length peters out after 50 although it goes up to 250 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  177853440 \n",
      "=================================================================\n",
      "Total params: 177,853,440\n",
      "Trainable params: 177,853,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 50 #use from chart above\n",
    "\n",
    "num_examples = 10000\n",
    "#num_examples = len(amharic_train.tweet)\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = tf.convert_to_tensor(amharic_train.label[:num_examples])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = tf.convert_to_tensor(amharic_dev.label[:num_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of positive examples:  0.504962962962963\n"
     ]
    }
   ],
   "source": [
    "#Let's look at class imbalance\n",
    "print('ratio of positive examples: ', np.sum(y_train==1)/len(y_train))\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From BERT_Fine_tuning Walkthrough Notebook/Session\n",
    "\n",
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs) #same as x_tiny example above, always set ouput to model acting on input\n",
    "\n",
    "    \n",
    "    #getting the CLS token, could change to bert_out[1]\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0]) \n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer',activation='relu')(classification_token)\n",
    "    \n",
    "    dropout = tf.keras.layers.Dropout(rate=0.1)(hidden)\n",
    "    \n",
    "    hidden2 = tf.keras.layers.Dense(hidden_size, name='hidden_layer2',activation='relu')(dropout)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden2)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Creating models and changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Following Model 1 from BERT_Fine_tuning walkthrough notebook with another dense layer and no dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 397s 144ms/step - loss: 0.6991 - accuracy: 0.4981 - val_loss: 0.6924 - val_accuracy: 0.5207\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 386s 143ms/step - loss: 0.6936 - accuracy: 0.5034 - val_loss: 0.6923 - val_accuracy: 0.5207\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 385s 143ms/step - loss: 0.6932 - accuracy: 0.5073 - val_loss: 0.6926 - val_accuracy: 0.5207\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 385s 143ms/step - loss: 0.6931 - accuracy: 0.5039 - val_loss: 0.6933 - val_accuracy: 0.4793\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 385s 143ms/step - loss: 0.6931 - accuracy: 0.5071 - val_loss: 0.6931 - val_accuracy: 0.5207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7facb6bfb460>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This took a long time, may want to increase batch_size for next run?\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50076824],\n",
       "       [0.50076824],\n",
       "       [0.50076824],\n",
       "       ...,\n",
       "       [0.50076824],\n",
       "       [0.50076824],\n",
       "       [0.50076824]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], ) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.50076824], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions) #All the same predictions :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6930848956108093 / Test accuracy: 0.5206666588783264\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7565c2f4f57a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_label\"] = np.nan\n",
      "<ipython-input-22-7565c2f4f57a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_stat\"] = predictions\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "#Creating some new columns & printing out a csv with the predicted labels\n",
    "amharic_train[\"predicted_label\"] = np.nan\n",
    "amharic_train[\"predicted_stat\"] = predictions\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] >= 0.5), \"predicted_label\"] = 1\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] < 0.5), \"predicted_label\"] = 0\n",
    "amharic_train.to_csv(\"Amharic_train_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Following Model 2 from BERT Walkthrough notebook\n",
    "Updating learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 413s 150ms/step - loss: 0.6996 - accuracy: 0.4995 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 404s 150ms/step - loss: 0.6945 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.5040\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 404s 150ms/step - loss: 0.6941 - accuracy: 0.4976 - val_loss: 0.6933 - val_accuracy: 0.5040\n",
      "Epoch 4/5\n",
      "2700/2700 [==============================] - 403s 149ms/step - loss: 0.6937 - accuracy: 0.5027 - val_loss: 0.6931 - val_accuracy: 0.5040\n",
      "Epoch 5/5\n",
      "2700/2700 [==============================] - 403s 149ms/step - loss: 0.6933 - accuracy: 0.5101 - val_loss: 0.6932 - val_accuracy: 0.4960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49768618],\n",
       "       [0.49768618],\n",
       "       [0.49768618],\n",
       "       ...,\n",
       "       [0.49768618],\n",
       "       [0.49768618],\n",
       "       [0.49768618]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same thing as above but change learning rate in Adam below, need to get fresh bert model\n",
    "try:\n",
    "    del classification_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del bert_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))\n",
    "\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)\n",
    "\n",
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask])\n",
    "\n",
    "\n",
    "#This looks a little worse, not sure why it's now predicting 54% consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6931948065757751 / Test accuracy: 0.4959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 177853440   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer2 (Dense)           (None, 200)          40200       hidden_layer[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 178,047,641\n",
      "Trainable params: 178,047,641\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Adding dropout in between two dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 165s 156ms/step - loss: 0.7145 - accuracy: 0.5055 - val_loss: 0.6931 - val_accuracy: 0.5083\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.6932 - accuracy: 0.5005 - val_loss: 0.6930 - val_accuracy: 0.5083\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.6928 - accuracy: 0.5167 - val_loss: 0.7164 - val_accuracy: 0.4917\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.6989 - accuracy: 0.5122 - val_loss: 0.6930 - val_accuracy: 0.5083\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 155s 155ms/step - loss: 0.6933 - accuracy: 0.4999 - val_loss: 0.6932 - val_accuracy: 0.5083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8be1a43f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This took a long time, may want to increase batch_size for next run?\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51710284],\n",
       "       [0.51710284],\n",
       "       [0.51710284],\n",
       "       ...,\n",
       "       [0.51710284],\n",
       "       [0.51710284],\n",
       "       [0.51710284]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], ) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.51710284], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Translate to English and then Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'veritas lux mea'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator.translate('veritas lux mea', src='la').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid source language",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-bbbcc9d06725>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hola'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sp'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtranslation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLANGCODES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid source language'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid source language"
     ]
    }
   ],
   "source": [
    "translation = translator.translate('hola',src='sp',dest='en')\n",
    "translation.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለማገዝ ብሞክር  እርስዎ የመፅሀፍ ቅዱሱ የዳዊት  ኦሮዮንን መንገድ በመምረጥዎ በይፋ ለመፃፍ ተገድጃለሁ  አሁንም መረጃው እንዴት እሱ ጋር ደረሰ የሚለውን ውትወታ ትተው በማንኛውም መንገድ ቀጥተኛ ምላሽ ይስጡኝ   ከመስከረም   ወዲያ አሁን ያለሁ መንግስት ህጋዊ የስራ ዘመኑ ስለሚያበቃ የባለአደራ መንግስት እንዲቋቋም ሆኖም አሁን ያለው የአብይ መንግስት አስፈፃሚ አካል የመንግስትን የእለ ተእለት ተግባራትን እየከወነ ምርጫ እስኪደረግ ለ አመት እ\n",
      "1\n",
      "እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረግ በመግለጫው ጠይቀዋል\n",
      "2\n",
      "የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የእውቀት ጎዳና አይኑ ያላየው ጆሮው ያልሰማው ልቡ ያላሰበው እውቀትና ብልሀት የለም\n",
      "3\n",
      "ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመልክ በመልኩ ሲቀዳበት የኖረ ከኢትዮጵያ አልፎ ለአለም ሲታደል የኖረውና የሚኖረው እውቀት የተገኘበት ታላቅ ነገድ ነው\n",
      "4\n",
      "ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ ህዝብ ላይ ብቻ ነውሌላው የትህነግን ባንዲራ ለብሶየኦነግን ባንዲራ እያውለበለበ የኢትዮጵያን ሰንደቅአላማ በእግሩ ረግጦታልጨርቅ ነው ብለው አቃጥለውታልቀዳደው ጥለውታል\n",
      "5\n",
      "በአማራ ህዝብ ግን ደም የገበረባትአጥንት የከሰከሰባት ሰንደቅአላማ ናትና በሀዘኑበደስታውበአደባባይና በሰልፍ በክብር ያውለበልባታልይፎክርባታልያዜምባታል\n",
      "6\n",
      "የኢትዮጵያ ብሄራዊ ቡድን በመቀሌ እስታዲየም ሲጫወት የገቡት ደጋፊዎች በፍሬ የሚቆጠሩ ናቸው ሊያውም ይዘውት የገቡት የትህነግን የደደቢት አርማ ይዘው እንደሆነ የቅርብ ጊዜ ትውስታ ነው\n",
      "7\n",
      "ዛሬም የአርሰናሉን ፔፔን የቶተንሀሙን ሰርጅ አውሪር ይዞ የገባው ኮትዲሾዋር ብሄራዊ ቡድን በአማራ ወጣቶች አስደማሚ ድጋፍ ዋልያዎቹ  አሸንፈው ወጥተዋል\n",
      "8\n",
      "ስማ ለምን ኡነተኛ ታሪክ አታወራም ለምንስ የሰው ታሪክ ትሰርቃላቹ የኢትዮጵያ ታሪክ የዳእማትና የአክሱም ካዛ ቀጥሎ የዛግወይ አገው ነው አማራ ከየት አመጣሀው እረ ተዉ ሼም ነው ሌብነት\n",
      "9\n",
      "ሀገራዊ ወግ ያለው ለሁሉ የሆነ አስገድዶ ሳይሆን በፍቅርና በልዩ ሰዋዊ አቀራረብ የሚቀርብ ትልቅ ሲሆን በፍቅር ስለፍቅር ከሁሉ የተዋለደ የወለደ የአለማቀፋዊነት ውሀ ልክ ያለው ወዘተ ህዝብ ነው\n"
     ]
    }
   ],
   "source": [
    "english = []\n",
    "num_examples = 10\n",
    "\n",
    "for i in range(num_examples):\n",
    "    print(i)\n",
    "    translation = translator.translate(str(amharic_data.tweet[i]), sdest='en')\n",
    "    english.append(translation.text)\n",
    "    print(translation.)\n",
    "    \n",
    "    \n",
    "amharic = amharic_data[:num_examples].copy()\n",
    "amharic['english'] = english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>label</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>በአማራ ህዝብ ግን ደም የገበረባትአጥንት የከሰከሰባት ሰንደቅአላማ ናትና ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>በአማራ ህዝብ ግን ደም የገበረባትአጥንት የከሰከሰባት ሰንደቅአላማ ናትና ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>የኢትዮጵያ ብሄራዊ ቡድን በመቀሌ እስታዲየም ሲጫወት የገቡት ደጋፊዎች በፍ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>የኢትዮጵያ ብሄራዊ ቡድን በመቀሌ እስታዲየም ሲጫወት የገቡት ደጋፊዎች በፍ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>ዛሬም የአርሰናሉን ፔፔን የቶተንሀሙን ሰርጅ አውሪር ይዞ የገባው ኮትዲሾዋ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ዛሬም የአርሰናሉን ፔፔን የቶተንሀሙን ሰርጅ አውሪር ይዞ የገባው ኮትዲሾዋ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ስማ ለምን ኡነተኛ ታሪክ አታወራም ለምንስ የሰው ታሪክ ትሰርቃላቹ የኢትዮ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ስማ ለምን ኡነተኛ ታሪክ አታወራም ለምንስ የሰው ታሪክ ትሰርቃላቹ የኢትዮ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>ሀገራዊ ወግ ያለው ለሁሉ የሆነ አስገድዶ ሳይሆን በፍቅርና በልዩ ሰዋዊ አ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ሀገራዊ ወግ ያለው ለሁሉ የሆነ አስገድዶ ሳይሆን በፍቅርና በልዩ ሰዋዊ አ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet subtask_a  label  \\\n",
       "0   0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...       NOT    0.0   \n",
       "1   1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...       NOT    0.0   \n",
       "2   2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...       NOT    0.0   \n",
       "3   3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...       NOT    0.0   \n",
       "4   4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...       OFF    1.0   \n",
       "5   5  በአማራ ህዝብ ግን ደም የገበረባትአጥንት የከሰከሰባት ሰንደቅአላማ ናትና ...       NOT    0.0   \n",
       "6   6  የኢትዮጵያ ብሄራዊ ቡድን በመቀሌ እስታዲየም ሲጫወት የገቡት ደጋፊዎች በፍ...       NOT    0.0   \n",
       "7   7  ዛሬም የአርሰናሉን ፔፔን የቶተንሀሙን ሰርጅ አውሪር ይዞ የገባው ኮትዲሾዋ...       NOT    0.0   \n",
       "8   8  ስማ ለምን ኡነተኛ ታሪክ አታወራም ለምንስ የሰው ታሪክ ትሰርቃላቹ የኢትዮ...       OFF    1.0   \n",
       "9   9  ሀገራዊ ወግ ያለው ለሁሉ የሆነ አስገድዶ ሳይሆን በፍቅርና በልዩ ሰዋዊ አ...       NOT    0.0   \n",
       "\n",
       "                                             english  \n",
       "0  አስቀድሜ ጥያቄዬ በጨዋነት በውስጥ መስመር እንዲደርስዎ አድርጌ ፍትህን ለ...  \n",
       "1  እነዚህን ወሳኝ ጉዳዮችን የሚያስፈፅም አካል እንዲቋቋምና ክትትል እንዲደረ...  \n",
       "2  የአማራ ህዝብ በአእምሮ ክንፉ ያልበረረበት ጥበብና ፍልስፍና ያልከፈተው የ...  \n",
       "3  ከአማራ ህዝብ የሀገሪቱ ዘርፈ ብዙ እውቀት መንጭቶ የሞላበትከሙላቱም በመል...  \n",
       "4  ዛሬ በየትኛውም መለኪያ ይሁን መመዘኛ ኢትዮጵያዊነት የሚንፀባረቀው በአማራ...  \n",
       "5  በአማራ ህዝብ ግን ደም የገበረባትአጥንት የከሰከሰባት ሰንደቅአላማ ናትና ...  \n",
       "6  የኢትዮጵያ ብሄራዊ ቡድን በመቀሌ እስታዲየም ሲጫወት የገቡት ደጋፊዎች በፍ...  \n",
       "7  ዛሬም የአርሰናሉን ፔፔን የቶተንሀሙን ሰርጅ አውሪር ይዞ የገባው ኮትዲሾዋ...  \n",
       "8  ስማ ለምን ኡነተኛ ታሪክ አታወራም ለምንስ የሰው ታሪክ ትሰርቃላቹ የኢትዮ...  \n",
       "9  ሀገራዊ ወግ ያለው ለሁሉ የሆነ አስገድዶ ሳይሆን በፍቅርና በልዩ ሰዋዊ አ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_csv(amharic_data, \"data/english_trans_amharic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into test/train again\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.8)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing\n",
    "\n",
    "max_length = 50 #use from chart above\n",
    "\n",
    "num_examples = 10000\n",
    "#num_examples = len(amharic_train.tweet)\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = tf.convert_to_tensor(amharic_train.label[:num_examples])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = tf.convert_to_tensor(amharic_dev.label[:num_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model()  \n",
    "#This took a long time, may want to increase batch_size for next run?\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], ) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not\n",
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
