# w266-project

## Abstract
Offensive language detection and other classification tasks often rely on large and well-labeled datasets. Low Resource Languages are languages for which large datasets may not exist or which may have incomplete or noisy datasets to train classification or other NLP tasks. In this project, we use pre-trained multilingual transformer models, mBERT and XLM-R, and a dataset in Amharic, a language spoken by around 50 million people in Ethiopia. In this work, we explore several techniques for utilizing and evaluating transfer learning and data-augmentation to increase performance in classifying social media posts as offensive or not in a Low Resource Language setting. The primary contributions we present in our paper are a data augmentation process and baselines that can provide the foundation for future work for researchers with the proper Amharic language background.


## Navigating the Files in this repository

|File | Description |
|:----|:------------|
