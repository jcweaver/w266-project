{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our predictions\n",
    "\n",
    "This is from a model trained on all of our langauges (English, Greek, Turkish, Arabic, Danish) and Amharic and then used to predict a test set of the Amharic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " false negatives count {} Unnamed: 0    1213\n",
      "PRED_NON      1213\n",
      "PRED_OFF      1213\n",
      "PRED          1213\n",
      "LABEL         1213\n",
      "TEXT          1213\n",
      "dtype: int64\n",
      " false positives count {} Unnamed: 0    1414\n",
      "PRED_NON      1414\n",
      "PRED_OFF      1414\n",
      "PRED          1414\n",
      "LABEL         1414\n",
      "TEXT          1414\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "adf = pd.read_csv(\"am_pred_data.csv\")\n",
    "\n",
    "correct_off = adf.loc[(adf[\"PRED\"] == 1) & (adf[\"LABEL\"] == 1)]\n",
    "correct_non = adf.loc[(adf[\"PRED\"] == 0) & (adf[\"LABEL\"] == 0)]\n",
    "\n",
    "false_non = adf.loc[(adf[\"PRED\"] == 0) & (adf[\"LABEL\"] == 1)]\n",
    "false_off = adf.loc[(adf[\"PRED\"] == 1) & (adf[\"LABEL\"] == 0)]\n",
    "print(\" false negatives count {}\", false_non.count())\n",
    "print(\" false positives count {}\", false_off.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a tokenizer to parse our FB comments out and see what we might be able to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ef3412cad322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_TYPE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xlm-roberta-base'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXLMRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXLMRobertaForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# xlm-roberta-large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "# xlm-roberta-large\n",
    "print('Loading XLMRoberta tokenizer...')\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "print('Tokenizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset( training_set ):\n",
    "    \n",
    "    _ids = []\n",
    "    \n",
    "    # look through all records\n",
    "    for index, row in training_set.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                    row['TEXT'], # Sentence to encode.\n",
    "                    add_special_tokens = False,      # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = 256,           # Pad or truncate.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "        # Add this example to our lists.\n",
    "        _ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_non_id = tokenize_dataset(false_non)\n",
    "false_off_id = tokenize_dataset(false_off)\n",
    "\n",
    "correct_off_id = tokenize_dataset(correct_off)\n",
    "correct_non_id = tokenize_dataset(correct_non)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get counts of all the words in these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "false_non_count = Counter()\n",
    "for xs in false_non_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        false_non_count[x] += 1\n",
    "\n",
    "false_off_count = Counter()\n",
    "for xs in false_off_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        false_off_count[x] += 1\n",
    "\n",
    "        \n",
    "correct_off_count = Counter()\n",
    "for xs in correct_off_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        correct_off_count[x] += 1\n",
    "\n",
    "        \n",
    "correct_non_count = Counter()\n",
    "for xs in correct_non_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        correct_non_count[x] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_to_false_non = false_non_count - false_off_count - correct_non_count - correct_off_count\n",
    "unique_to_false_non.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_false_off = false_off_count - false_non_count - correct_non_count - correct_off_count\n",
    "unique_to_false_off.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_to_off = correct_off_count - false_off_count - false_non_count - correct_non_count\n",
    "unique_to_off.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_dataset( ids ):\n",
    "\n",
    "    _texts = []\n",
    "\n",
    "    # look through all records\n",
    "    for row in ids:\n",
    "        _texts.append(tokenizer.decode(row))\n",
    "\n",
    "    return _texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's most common in tweets identified as 'offensive' but which are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_off_words = detokenize_dataset([k for k,v in unique_to_false_off.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "የሩሳሌም : Of Jerusalem\n",
    "\n",
    "\n",
    "ይመልከቱ : look at, behold, browse\n",
    "\n",
    "A little hard to see what might be offensive without context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's most common in tweets identified as 'inoffensive' but which are offensive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_non_words = detokenize_dataset([k for k,v in unique_to_false_non.most_common()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "እንቅስቃሴ : life, lives, movement\n",
    "\n",
    "ኢየሱስ : jesus\n",
    "\n",
    "ተከታታይ : successive, following (one another)\n",
    "\n",
    "\n",
    "So this is pretty hard to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one of these: የወያኔ\n",
    "\n",
    "Google has it as \"Oh my gosh\"\n",
    "\n",
    "[This](https://dictionary.abyssinica.com/%E1%8B%A8%E1%8B%88%E1%8B%AB%E1%8A%94) Amharic dictionary on the other hand, has it as \"tribal movement in Tigray, northern Ethiopia\", the site of a civil war."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe we could think of ways to reweight some of these words or phrases around these words?\n",
    "\n",
    "Maybe an n-gram model of creating synthetic tweets that use phrases we know *should* be inoffensive or offensive?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After speaking with Zach\n",
    "\n",
    "We need a dataset that's isolated from our 'key term' identification to test against. Make that by grabbing 1000 examples, and training everything without those examples in the pipeline, and then doing the detection again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "am_split = pd.read_csv(\"data/Amharic/amharic.csv\")\n",
    "\n",
    "am_train, am_test_reserve = model_selection.train_test_split(am_split, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_test_reserve.to_csv(\"data/Amharic/amharic_test_reserve_500.csv\", index=False)\n",
    "am_train.to_csv(\"data/Amharic/amharic_train_29.5k.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
