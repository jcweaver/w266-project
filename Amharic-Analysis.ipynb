{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f16b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39031901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " false negatives count {} Unnamed: 0    1213\n",
      "PRED_NON      1213\n",
      "PRED_OFF      1213\n",
      "PRED          1213\n",
      "LABEL         1213\n",
      "TEXT          1213\n",
      "dtype: int64\n",
      " false positives count {} Unnamed: 0    1414\n",
      "PRED_NON      1414\n",
      "PRED_OFF      1414\n",
      "PRED          1414\n",
      "LABEL         1414\n",
      "TEXT          1414\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "adf = pd.read_csv(\"am_pred_data.csv\")\n",
    "\n",
    "correct_off = adf.loc[(adf[\"PRED\"] == 1) & (adf[\"LABEL\"] == 1)]\n",
    "correct_non = adf.loc[(adf[\"PRED\"] == 0) & (adf[\"LABEL\"] == 0)]\n",
    "\n",
    "false_non = adf.loc[(adf[\"PRED\"] == 0) & (adf[\"LABEL\"] == 1)]\n",
    "false_off = adf.loc[(adf[\"PRED\"] == 1) & (adf[\"LABEL\"] == 0)]\n",
    "print(\" false negatives count {}\", false_non.count())\n",
    "print(\" false positives count {}\", false_off.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a92bb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLMRoberta tokenizer...\n",
      "Tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_TYPE = 'xlm-roberta-base'\n",
    "\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "\n",
    "# xlm-roberta-large\n",
    "print('Loading XLMRoberta tokenizer...')\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "print('Tokenizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e41f18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset( training_set ):\n",
    "    \n",
    "    _ids = []\n",
    "    \n",
    "    # look through all records\n",
    "    for index, row in training_set.iterrows():\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                    row['TEXT'], # Sentence to encode.\n",
    "                    add_special_tokens = False,      # Add '[CLS]' and '[SEP]'\n",
    "                    max_length = 256,           # Pad or truncate.\n",
    "                    pad_to_max_length = True,\n",
    "                    return_attention_mask = True,   # Construct attn. masks.\n",
    "                    return_tensors = 'pt',          # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "        # Add this example to our lists.\n",
    "        _ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    return _ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6c8490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/joshnoble/.local/share/virtualenvs/joshnoble-TX97Gyyd/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2105: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "false_non_id = tokenize_dataset(false_non)\n",
    "false_off_id = tokenize_dataset(false_off)\n",
    "\n",
    "correct_off_id = tokenize_dataset(correct_off)\n",
    "correct_non_id = tokenize_dataset(correct_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5ca8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "false_non_count = Counter()\n",
    "for xs in false_non_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        false_non_count[x] += 1\n",
    "\n",
    "false_off_count = Counter()\n",
    "for xs in false_off_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        false_off_count[x] += 1\n",
    "\n",
    "        \n",
    "correct_off_count = Counter()\n",
    "for xs in correct_off_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        correct_off_count[x] += 1\n",
    "\n",
    "        \n",
    "correct_non_count = Counter()\n",
    "for xs in correct_non_id:\n",
    "    for x in set(xs[0].tolist()):\n",
    "        correct_non_count[x] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a1ee8f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(77945, 3),\n",
       " (74684, 3),\n",
       " (140340, 2),\n",
       " (175777, 2),\n",
       " (222768, 2),\n",
       " (162872, 2),\n",
       " (242454, 2),\n",
       " (235538, 2),\n",
       " (243857, 2),\n",
       " (238626, 2),\n",
       " (83899, 1),\n",
       " (242463, 1),\n",
       " (217511, 1),\n",
       " (137249, 1),\n",
       " (247417, 1),\n",
       " (241915, 1),\n",
       " (229567, 1),\n",
       " (235388, 1),\n",
       " (215889, 1),\n",
       " (144924, 1),\n",
       " (223128, 1),\n",
       " (181020, 1),\n",
       " (221404, 1),\n",
       " (189512, 1),\n",
       " (216661, 1),\n",
       " (103114, 1),\n",
       " (221428, 1),\n",
       " (224819, 1),\n",
       " (217961, 1),\n",
       " (38018, 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_false_non = false_non_count - false_off_count - correct_non_count - correct_off_count\n",
    "unique_to_false_non.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76cb3854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(217042, 2),\n",
       " (242046, 2),\n",
       " (223589, 2),\n",
       " (173203, 1),\n",
       " (217512, 1),\n",
       " (247088, 1),\n",
       " (230717, 1),\n",
       " (224896, 1),\n",
       " (221652, 1),\n",
       " (196936, 1),\n",
       " (245908, 1),\n",
       " (1146, 1),\n",
       " (28677, 1),\n",
       " (246316, 1),\n",
       " (238201, 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_false_off = false_off_count - false_non_count - correct_non_count - correct_off_count\n",
    "unique_to_false_off.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c937d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(97081, 113),\n",
       " (78906, 34),\n",
       " (211576, 32),\n",
       " (128103, 23),\n",
       " (153196, 23),\n",
       " (70337, 22),\n",
       " (176415, 21),\n",
       " (134567, 19),\n",
       " (151666, 19),\n",
       " (112944, 19),\n",
       " (101616, 18),\n",
       " (49648, 18),\n",
       " (216110, 17),\n",
       " (225551, 15),\n",
       " (199683, 15),\n",
       " (59581, 14),\n",
       " (94914, 13),\n",
       " (160215, 12),\n",
       " (73845, 11),\n",
       " (120083, 10),\n",
       " (73585, 10),\n",
       " (121015, 9),\n",
       " (190886, 9),\n",
       " (191878, 9),\n",
       " (168538, 8),\n",
       " (156828, 8),\n",
       " (177838, 8),\n",
       " (244345, 7),\n",
       " (229217, 7),\n",
       " (242478, 7),\n",
       " (189821, 7),\n",
       " (178283, 7),\n",
       " (167723, 7),\n",
       " (103047, 6),\n",
       " (166870, 6),\n",
       " (183630, 6),\n",
       " (121702, 6),\n",
       " (220753, 6),\n",
       " (209005, 6),\n",
       " (212735, 6),\n",
       " (233755, 6),\n",
       " (204634, 6),\n",
       " (244633, 6),\n",
       " (124449, 6),\n",
       " (215108, 6),\n",
       " (99177, 6),\n",
       " (222748, 5),\n",
       " (168112, 5),\n",
       " (113922, 5),\n",
       " (86838, 5),\n",
       " (162354, 5),\n",
       " (124956, 5),\n",
       " (59876, 5),\n",
       " (182035, 5),\n",
       " (239082, 5),\n",
       " (211445, 4),\n",
       " (63853, 4),\n",
       " (199345, 4),\n",
       " (160815, 4),\n",
       " (123602, 4),\n",
       " (155165, 4),\n",
       " (201760, 4),\n",
       " (242410, 4),\n",
       " (182168, 4),\n",
       " (45572, 3),\n",
       " (112346, 3),\n",
       " (161605, 3),\n",
       " (177970, 3),\n",
       " (184111, 3),\n",
       " (239086, 3),\n",
       " (136779, 3),\n",
       " (145565, 3),\n",
       " (167581, 3),\n",
       " (105413, 3),\n",
       " (122864, 3),\n",
       " (157522, 3),\n",
       " (230446, 3),\n",
       " (230794, 3),\n",
       " (167979, 2),\n",
       " (195849, 2),\n",
       " (119631, 2),\n",
       " (156070, 2),\n",
       " (238625, 2),\n",
       " (191251, 2),\n",
       " (239465, 2),\n",
       " (182757, 2),\n",
       " (236120, 2),\n",
       " (211105, 2),\n",
       " (152824, 2),\n",
       " (185204, 2),\n",
       " (202024, 2),\n",
       " (235767, 2),\n",
       " (214170, 2),\n",
       " (169128, 2),\n",
       " (196655, 2),\n",
       " (223647, 2),\n",
       " (208810, 2),\n",
       " (189948, 2),\n",
       " (174360, 2),\n",
       " (156945, 2),\n",
       " (200399, 2),\n",
       " (122109, 1),\n",
       " (222707, 1),\n",
       " (206073, 1),\n",
       " (242041, 1),\n",
       " (182152, 1),\n",
       " (221458, 1),\n",
       " (121873, 1),\n",
       " (126399, 1),\n",
       " (242763, 1),\n",
       " (200735, 1),\n",
       " (119910, 1),\n",
       " (104106, 1),\n",
       " (199333, 1),\n",
       " (158451, 1),\n",
       " (249288, 1),\n",
       " (227573, 1),\n",
       " (188661, 1),\n",
       " (229978, 1),\n",
       " (240051, 1),\n",
       " (206545, 1),\n",
       " (221776, 1),\n",
       " (150940, 1),\n",
       " (143159, 1),\n",
       " (190882, 1),\n",
       " (221427, 1),\n",
       " (139970, 1),\n",
       " (232872, 1),\n",
       " (224630, 1),\n",
       " (244338, 1),\n",
       " (204480, 1),\n",
       " (244632, 1),\n",
       " (221184, 1),\n",
       " (223337, 1),\n",
       " (244631, 1),\n",
       " (205259, 1),\n",
       " (182555, 1),\n",
       " (187217, 1),\n",
       " (245387, 1),\n",
       " (227562, 1),\n",
       " (228148, 1),\n",
       " (197784, 1),\n",
       " (245726, 1),\n",
       " (138482, 1),\n",
       " (167726, 1),\n",
       " (113443, 1),\n",
       " (246177, 1),\n",
       " (233395, 1),\n",
       " (17762, 1),\n",
       " (197688, 1),\n",
       " (233476, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_to_off = correct_off_count - false_off_count - false_non_count - correct_non_count\n",
    "unique_to_off.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4249d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_dataset( ids ):\n",
    "\n",
    "    _texts = []\n",
    "\n",
    "    # look through all records\n",
    "    for row in ids:\n",
    "        _texts.append(tokenizer.decode(row))\n",
    "\n",
    "    return _texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c819f",
   "metadata": {},
   "source": [
    "So what's most common in tweets identified as 'offensive' but which are not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcc06db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['የሩሳሌም', 'ይመልከቱ', '•••', 'ዲስ', 'እስረኞች', 'ዯ', 'አውሮፕላን', 'እጥረት', 'ፓርቲው', 'ዴሞክራሲያዊ', 'ሇ', '!!', '¤', 'ቑ', '፭']\n"
     ]
    }
   ],
   "source": [
    "print(detokenize_dataset([k for k,v in unique_to_false_off.most_common()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa352f",
   "metadata": {},
   "source": [
    "የሩሳሌም : Of Jerusalem\n",
    "\n",
    "\n",
    "ይመልከቱ : look at, behold, browse\n",
    "\n",
    "A little hard to see what might be offensive without context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decc459c",
   "metadata": {},
   "source": [
    "So what's most common in tweets identified as 'inoffensive' but which are offensive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b59947e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['እንቅስቃሴ', 'ኢየሱስ', 'ጋዜጣ', 'ምክትል', 'ኪዳን', 'እስራኤል', 'መዋቅር', 'ተሰኘው', 'ኰ', 'አስተያየቶች', 'በርካታ', 'ርቀት', 'አስመልክቶ', 'ጥቃት', 'ቌ', 'ዋሽንግተን', 'ተከታታይ', 'ጋብቻ', 'የምግብ', '፪', 'ተወላጆች', 'የገንዘብ', 'ጠቅላላ', 'ክሲ', 'ቅዳሜ', 'ፕሮግራም', 'ይናገራሉ', 'ቕ', 'ጀርመን', '¿']\n"
     ]
    }
   ],
   "source": [
    "print(detokenize_dataset([k for k,v in unique_to_false_non.most_common()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64367bcf",
   "metadata": {},
   "source": [
    "እንቅስቃሴ : life, lives, movement\n",
    "\n",
    "ኢየሱስ : jesus\n",
    "\n",
    "ተከታታይ : successive, following (one another)\n",
    "\n",
    "\n",
    "So this is pretty hard to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbd4ad62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['አንተ', 'ለህ', 'ደደ', 'ነህ', 'ወራ', 'አት', 'ታም', 'እሱ', 'ረኛ', 'አፍ', 'ወያኔ', 'ልጅ', 'እንዴ', 'ወሬ', 'ኦነግ', 'የምት', 'ጉ', 'ክራ', 'መጥ', 'ንቅ', 'በላ', 'ፍር', 'ለሽ', 'ትግሬ', 'ዣ', 'የወያኔ', 'ርካ', 'ዠ', 'አሸባሪ', 'ጥላቻ', 'በቀለ', 'ዳም', 'ነገሩ', 'ደል', 'ጉድ', 'መስል', 'ድብ', 'ናገረው', 'ርጉ', 'አሰፋ', 'ሰይጣን', 'በሉ', 'ጬ', 'ማሪ', 'ግፍ', 'ዲ', 'ቃወም', 'ወያኔ', 'እያለ', 'ሽ', 'የሞ', 'ካን', 'ፈው', 'ጥሩ', 'ወለድ', 'የሚባለው', 'ቆ', 'አሸ', 'ለቅ', 'ሸ', 'ፏ', 'ይባላል', 'ስብስብ', 'የመጀመሪያው', 'ሰው', 'ከራ', 'ሰባት', 'ዷ', 'ላቸውን', 'ቀርቶ', 'ተራ', 'እንደነበር', 'እዩ', 'አጋ', 'የር', 'አቅ', 'ግማሽ', 'ሌሊት', 'ቁም', 'ራችሁ', 'ወንጀል', 'ርነት', 'ባሻገር', 'ዱት', 'ወንዝ', 'ሻሻ', 'ዘጠኝ', 'ስለተ', 'ለጥ', 'ቃቸው', 'የተባለ', 'ኢሳያስ', 'እዚያ', 'አበ', 'ሰኞ', 'ተስፋዬ', 'ሰበር', 'ሮቹ', 'ወሰን', 'ዎችና', 'ስርጭት', 'ወርቅ', 'ምንጮች', 'ፌስቡክ', 'ምንድነው', 'ራፊ', 'ታማኝ', 'ስማ', 'መኪና', 'መጥፎ', 'ለዚህም', 'ብሪ', 'ጋ', 'እንደ', 'ውጪ', 'ኴ', 'ቀዳሚ', 'ከዚያም', 'የግለሰቦች', 'ማክሰኞ', 'አስገራሚ', 'መለከቱ', 'ይወ', 'ዘገባ', 'ታስ', 'ሰራዊት', 'መንፈስ', 'እርሳቸው', 'ለወጥ', '∞', 'በቅርብ', 'ኄ', 'ወስድ', 'ሁለተኛው', 'ሿ', 'የእርስዎ', 'አርበኞች', 'እንግሊዝ', 'ኟ', 'ደራሲ', 'የኢትዮጵያ', 'መጋቢት', 'ጒ', 'ሽብር', '_____', 'ግንቦት', 'ኲ', 'ፈረንሳይ', '•', 'ኤም', 'ቻይና']\n"
     ]
    }
   ],
   "source": [
    "print(detokenize_dataset([k for k,v in unique_to_off.most_common()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32932ea4",
   "metadata": {},
   "source": [
    "Let's pick one of these: የወያኔ\n",
    "\n",
    "Google has it as \"Oh my gosh\"\n",
    "\n",
    "[This](https://dictionary.abyssinica.com/%E1%8B%A8%E1%8B%88%E1%8B%AB%E1%8A%94) Amharic dictionary on the other hand, has it as \"tribal movement in Tigray, northern Ethiopia\", the site of a civil war."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6fa83",
   "metadata": {},
   "source": [
    "## Maybe we could think of ways to reweight some of these words or phrases around these words?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
