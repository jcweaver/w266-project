{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilingual BERT Approach on English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import transformers\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU presence\n",
    "tf.config.list_physical_devices('GPU')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  \n",
       "0           UNT       NaN  \n",
       "1           TIN       IND  \n",
       "2           NaN       NaN  \n",
       "3           UNT       NaN  \n",
       "4           NaN       NaN  \n",
       "...         ...       ...  \n",
       "13235       TIN       IND  \n",
       "13236       NaN       NaN  \n",
       "13237       TIN       OTH  \n",
       "13238       UNT       NaN  \n",
       "13239       NaN       NaN  \n",
       "\n",
       "[13240 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english = pd.read_csv(\"data/olid/olid-training-v1.0.tsv\", sep =\"\\t\")\n",
    "english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resettting df to be amharic_data to be easier to use remaining code\n",
    "amharic_data = english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OTH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "0      86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1      90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2      16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3      62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4      43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "13235  95338  @USER Sometimes I get strong vibes from people...       OFF   \n",
       "13236  67210  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...       NOT   \n",
       "13237  82921  @USER And why report this garbage.  We don't g...       OFF   \n",
       "13238  27429                                        @USER Pussy       OFF   \n",
       "13239  46552  #Spanishrevenge vs. #justice #HumanRights and ...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  label  \n",
       "0           UNT       NaN    1.0  \n",
       "1           TIN       IND    1.0  \n",
       "2           NaN       NaN    0.0  \n",
       "3           UNT       NaN    1.0  \n",
       "4           NaN       NaN    0.0  \n",
       "...         ...       ...    ...  \n",
       "13235       TIN       IND    1.0  \n",
       "13236       NaN       NaN    0.0  \n",
       "13237       TIN       OTH    1.0  \n",
       "13238       UNT       NaN    1.0  \n",
       "13239       NaN       NaN    0.0  \n",
       "\n",
       "[13240 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating new column with 0/1\n",
    "amharic_data[\"label\"] = np.nan\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"OFF\"), \"label\"] = 1\n",
    "amharic_data.loc[(amharic_data[\"subtask_a\"] == \"NOT\"), \"label\"] = 0\n",
    "amharic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10592, 6)\n",
      "(1324, 6)\n",
      "(1324, 6)\n"
     ]
    }
   ],
   "source": [
    "#load our tuned Amharic dataset\n",
    "amharic_train, amharic_test = train_test_split(amharic_data, train_size=0.8)\n",
    "amharic_test, amharic_dev = train_test_split(amharic_test, train_size=0.5)\n",
    "print(amharic_train.shape)\n",
    "print(amharic_test.shape)\n",
    "print(amharic_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>54676</td>\n",
       "      <td>Lyin #BrettKavanaugh &amp;amp; His Sexual Assault ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>54306</td>\n",
       "      <td>@USER @USER Hillary? Why is she on the social ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10432</th>\n",
       "      <td>85283</td>\n",
       "      <td>@USER Antifa is a domestic terror organisation...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5645</th>\n",
       "      <td>44586</td>\n",
       "      <td>@USER @USER @USER There are people and groups ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>35695</td>\n",
       "      <td>@USER It only hangs in the balance because of ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>94903</td>\n",
       "      <td>@USER @USER What about the game clincher. He b...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9718</th>\n",
       "      <td>60107</td>\n",
       "      <td>@USER Fake News</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>31247</td>\n",
       "      <td>@USER @USER @USER @USER @USER @USER @USER And ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6549</th>\n",
       "      <td>53573</td>\n",
       "      <td>@USER @USER Now I see why the Conservatives lo...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>12559</td>\n",
       "      <td>@USER THAT'S NOTHING NEW SHE WAS GOING TO VOTE...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10592 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet subtask_a  \\\n",
       "12662  54676  Lyin #BrettKavanaugh &amp; His Sexual Assault ...       NOT   \n",
       "1456   54306  @USER @USER Hillary? Why is she on the social ...       OFF   \n",
       "10432  85283  @USER Antifa is a domestic terror organisation...       NOT   \n",
       "5645   44586  @USER @USER @USER There are people and groups ...       NOT   \n",
       "2240   35695  @USER It only hangs in the balance because of ...       NOT   \n",
       "...      ...                                                ...       ...   \n",
       "4025   94903  @USER @USER What about the game clincher. He b...       NOT   \n",
       "9718   60107                                    @USER Fake News       OFF   \n",
       "10884  31247  @USER @USER @USER @USER @USER @USER @USER And ...       NOT   \n",
       "6549   53573  @USER @USER Now I see why the Conservatives lo...       NOT   \n",
       "100    12559  @USER THAT'S NOTHING NEW SHE WAS GOING TO VOTE...       NOT   \n",
       "\n",
       "      subtask_b subtask_c  label  \n",
       "12662       NaN       NaN    0.0  \n",
       "1456        TIN       IND    1.0  \n",
       "10432       NaN       NaN    0.0  \n",
       "5645        NaN       NaN    0.0  \n",
       "2240        NaN       NaN    0.0  \n",
       "...         ...       ...    ...  \n",
       "4025        NaN       NaN    0.0  \n",
       "9718        UNT       NaN    1.0  \n",
       "10884       NaN       NaN    0.0  \n",
       "6549        NaN       NaN    0.0  \n",
       "100         NaN       NaN    0.0  \n",
       "\n",
       "[10592 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amharic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in amharic_train.tweet]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)\n",
    "\n",
    "\n",
    "#It looks like the max length peters out after 50 although it goes up to 250 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = 50 #use from chart above\n",
    "\n",
    "num_examples = 1000\n",
    "#num_examples = len(amharic_train.tweet)\n",
    "\n",
    "x_train = tokenizer([x for x in amharic_train.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_train = tf.convert_to_tensor(amharic_train.label[:num_examples])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_dev = tokenizer([x for x in amharic_dev.tweet][:num_examples], \n",
    "              max_length=max_length,\n",
    "              truncation=True,\n",
    "              padding='max_length', \n",
    "              return_tensors='tf')\n",
    "y_dev = tf.convert_to_tensor(amharic_dev.label[:num_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio of positive examples:  0.504962962962963\n"
     ]
    }
   ],
   "source": [
    "#Let's look at class imbalance\n",
    "print('ratio of positive examples: ', np.sum(y_train==1)/len(y_train))\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0],\n",
       "       [101, 100, 100, ...,   0,   0,   0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(27000, 50), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From BERT_Fine_tuning Walkthrough Notebook/Session\n",
    "\n",
    "def create_classification_model(hidden_size = 200, \n",
    "                                train_layers = -1, \n",
    "                                optimizer=tf.keras.optimizers.Adam()):\n",
    "    \"\"\"\n",
    "    Build a simple classification model with BERT. Let's keep it simple and don't add dropout, layer norms, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "    token_type_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "    attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "\n",
    "    bert_inputs = {'input_ids': input_ids,\n",
    "                  'token_type_ids': token_type_ids,\n",
    "                  'attention_mask': attention_mask}\n",
    "\n",
    "\n",
    "    #restrict training to the train_layers outer transformer layers\n",
    "    if not train_layers == -1:\n",
    "\n",
    "            retrain_layers = []\n",
    "\n",
    "            for retrain_layer_number in range(train_layers):\n",
    "\n",
    "                layer_code = '_' + str(11 - retrain_layer_number)\n",
    "                retrain_layers.append(layer_code)\n",
    "\n",
    "            for w in bert_model.weights:\n",
    "                if not any([x in w.name for x in retrain_layers]):\n",
    "                    w._trainable = False\n",
    "\n",
    "\n",
    "    bert_out = bert_model(bert_inputs) #same as x_tiny example above, always set ouput to model acting on input\n",
    "\n",
    "    \n",
    "    #getting the CLS token, could change to bert_out[1]\n",
    "    classification_token = tf.keras.layers.Lambda(lambda x: x[:,0,:], name='get_first_vector')(bert_out[0]) \n",
    "\n",
    "\n",
    "    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer',activation='relu')(classification_token)\n",
    "    \n",
    "    hidden2 = tf.keras.layers.Dense(hidden_size, name='hidden_layer2',activation='relu')(hidden)\n",
    "\n",
    "    classification = tf.keras.layers.Dense(1, activation='sigmoid',name='classification_layer')(hidden2)\n",
    "\n",
    "    classification_model = tf.keras.Model(inputs=[input_ids, token_type_ids, attention_mask], \n",
    "                                          outputs=[classification])\n",
    "    \n",
    "    classification_model.compile(optimizer=optimizer,\n",
    "                            loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                            metrics='accuracy')\n",
    "\n",
    "\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Creating models and changing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1: Following Model 1 from BERT_Fine_tuning walkthrough notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 28s 197ms/step - loss: 0.6163 - accuracy: 0.7018 - val_loss: 0.6307 - val_accuracy: 0.6610\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 0.5835 - accuracy: 0.7128 - val_loss: 0.6108 - val_accuracy: 0.7100\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 0.5589 - accuracy: 0.7598 - val_loss: 0.6274 - val_accuracy: 0.6830\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 0.6021 - accuracy: 0.7364 - val_loss: 0.6475 - val_accuracy: 0.6610\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 18s 181ms/step - loss: 0.6177 - accuracy: 0.6960 - val_loss: 0.6370 - val_accuracy: 0.6770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f89d2307fa0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This took a long time, may want to increase batch_size for next run?\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27059484],\n",
       "       [0.2696303 ],\n",
       "       [0.2696888 ],\n",
       "       [0.2692753 ],\n",
       "       [0.26968333],\n",
       "       [0.9643061 ],\n",
       "       [0.26969713],\n",
       "       [0.2697132 ],\n",
       "       [0.2701217 ],\n",
       "       [0.26889256],\n",
       "       [0.27006915],\n",
       "       [0.26882908],\n",
       "       [0.26903936],\n",
       "       [0.26970384],\n",
       "       [0.26948008],\n",
       "       [0.26942965],\n",
       "       [0.2696312 ],\n",
       "       [0.2696685 ],\n",
       "       [0.26856795],\n",
       "       [0.26994434],\n",
       "       [0.26959875],\n",
       "       [0.26949593],\n",
       "       [0.26957875],\n",
       "       [0.2696573 ],\n",
       "       [0.26912746],\n",
       "       [0.27006558],\n",
       "       [0.26856795],\n",
       "       [0.26938304],\n",
       "       [0.2698845 ],\n",
       "       [0.26958004],\n",
       "       [0.26924857],\n",
       "       [0.2691555 ],\n",
       "       [0.26970997],\n",
       "       [0.2691587 ],\n",
       "       [0.269322  ],\n",
       "       [0.26856795],\n",
       "       [0.2702536 ],\n",
       "       [0.26932856],\n",
       "       [0.26963404],\n",
       "       [0.26961792],\n",
       "       [0.2692421 ],\n",
       "       [0.26887497],\n",
       "       [0.27001   ],\n",
       "       [0.2695263 ],\n",
       "       [0.26950788],\n",
       "       [0.2698029 ],\n",
       "       [0.26962775],\n",
       "       [0.26937124],\n",
       "       [0.26943278],\n",
       "       [0.26940218],\n",
       "       [0.26966494],\n",
       "       [0.2694718 ],\n",
       "       [0.26929158],\n",
       "       [0.26901948],\n",
       "       [0.26945108],\n",
       "       [0.26976612],\n",
       "       [0.27054313],\n",
       "       [0.27031472],\n",
       "       [0.2695081 ],\n",
       "       [0.26938358],\n",
       "       [0.26945677],\n",
       "       [0.269874  ],\n",
       "       [0.27008647],\n",
       "       [0.2704292 ],\n",
       "       [0.26968986],\n",
       "       [0.26951507],\n",
       "       [0.26978347],\n",
       "       [0.26941344],\n",
       "       [0.26911524],\n",
       "       [0.26952112],\n",
       "       [0.96425927],\n",
       "       [0.26914328],\n",
       "       [0.26938358],\n",
       "       [0.2693451 ],\n",
       "       [0.26924035],\n",
       "       [0.26958188],\n",
       "       [0.26856795],\n",
       "       [0.26960644],\n",
       "       [0.2696501 ],\n",
       "       [0.26974756],\n",
       "       [0.26943874],\n",
       "       [0.26962993],\n",
       "       [0.26982442],\n",
       "       [0.2695399 ],\n",
       "       [0.26933178],\n",
       "       [0.9607397 ],\n",
       "       [0.26929736],\n",
       "       [0.2699849 ],\n",
       "       [0.2694149 ],\n",
       "       [0.26994514],\n",
       "       [0.2693827 ],\n",
       "       [0.26904702],\n",
       "       [0.26952195],\n",
       "       [0.2692319 ],\n",
       "       [0.27002034],\n",
       "       [0.26951844],\n",
       "       [0.26952976],\n",
       "       [0.26959208],\n",
       "       [0.26920846],\n",
       "       [0.2696144 ],\n",
       "       [0.9656086 ],\n",
       "       [0.26961234],\n",
       "       [0.2697608 ],\n",
       "       [0.96471447],\n",
       "       [0.26966882],\n",
       "       [0.26988456],\n",
       "       [0.26896748],\n",
       "       [0.269376  ],\n",
       "       [0.26972002],\n",
       "       [0.26959947],\n",
       "       [0.26951787],\n",
       "       [0.269366  ],\n",
       "       [0.27006373],\n",
       "       [0.27065745],\n",
       "       [0.2692264 ],\n",
       "       [0.26946864],\n",
       "       [0.26948148],\n",
       "       [0.2696564 ],\n",
       "       [0.270849  ],\n",
       "       [0.26965114],\n",
       "       [0.27025193],\n",
       "       [0.26961914],\n",
       "       [0.26927724],\n",
       "       [0.2691821 ],\n",
       "       [0.26940593],\n",
       "       [0.2687848 ],\n",
       "       [0.26856795],\n",
       "       [0.96303445],\n",
       "       [0.26960468],\n",
       "       [0.26964018],\n",
       "       [0.27015653],\n",
       "       [0.269545  ],\n",
       "       [0.2694936 ],\n",
       "       [0.26964098],\n",
       "       [0.27034485],\n",
       "       [0.26922768],\n",
       "       [0.2700152 ],\n",
       "       [0.27281344],\n",
       "       [0.26984984],\n",
       "       [0.2700995 ],\n",
       "       [0.26973036],\n",
       "       [0.26961026],\n",
       "       [0.26930496],\n",
       "       [0.26940274],\n",
       "       [0.26947248],\n",
       "       [0.26947194],\n",
       "       [0.26923263],\n",
       "       [0.26964545],\n",
       "       [0.27016804],\n",
       "       [0.26949218],\n",
       "       [0.2696834 ],\n",
       "       [0.26924884],\n",
       "       [0.2695677 ],\n",
       "       [0.2699075 ],\n",
       "       [0.26888964],\n",
       "       [0.26972982],\n",
       "       [0.2745173 ],\n",
       "       [0.26944903],\n",
       "       [0.26919058],\n",
       "       [0.9554304 ],\n",
       "       [0.26919886],\n",
       "       [0.26987273],\n",
       "       [0.2694187 ],\n",
       "       [0.2697318 ],\n",
       "       [0.2687808 ],\n",
       "       [0.26944476],\n",
       "       [0.26931408],\n",
       "       [0.26989442],\n",
       "       [0.26959413],\n",
       "       [0.26959   ],\n",
       "       [0.26997846],\n",
       "       [0.2693822 ],\n",
       "       [0.269908  ],\n",
       "       [0.26929784],\n",
       "       [0.26968217],\n",
       "       [0.26955855],\n",
       "       [0.26967734],\n",
       "       [0.9623409 ],\n",
       "       [0.26913646],\n",
       "       [0.96354455],\n",
       "       [0.26937643],\n",
       "       [0.2710021 ],\n",
       "       [0.268998  ],\n",
       "       [0.26883084],\n",
       "       [0.26970747],\n",
       "       [0.2696158 ],\n",
       "       [0.2695646 ],\n",
       "       [0.26922375],\n",
       "       [0.270213  ],\n",
       "       [0.2697206 ],\n",
       "       [0.26963043],\n",
       "       [0.26940215],\n",
       "       [0.2694856 ],\n",
       "       [0.26911068],\n",
       "       [0.26883557],\n",
       "       [0.26906562],\n",
       "       [0.26989737],\n",
       "       [0.2699521 ],\n",
       "       [0.2694566 ],\n",
       "       [0.27006847],\n",
       "       [0.26948634],\n",
       "       [0.26975536],\n",
       "       [0.26991567],\n",
       "       [0.26932028],\n",
       "       [0.2691869 ],\n",
       "       [0.2695566 ],\n",
       "       [0.2695693 ],\n",
       "       [0.26949272],\n",
       "       [0.26980665],\n",
       "       [0.26946163],\n",
       "       [0.26963776],\n",
       "       [0.26985124],\n",
       "       [0.26938874],\n",
       "       [0.26856795],\n",
       "       [0.2693747 ],\n",
       "       [0.26926756],\n",
       "       [0.9636564 ],\n",
       "       [0.26966098],\n",
       "       [0.26974756],\n",
       "       [0.26963606],\n",
       "       [0.26998687],\n",
       "       [0.26949388],\n",
       "       [0.26957658],\n",
       "       [0.26951978],\n",
       "       [0.2693981 ],\n",
       "       [0.2702018 ],\n",
       "       [0.26923922],\n",
       "       [0.26985478],\n",
       "       [0.26938805],\n",
       "       [0.2699913 ],\n",
       "       [0.27015543],\n",
       "       [0.26920575],\n",
       "       [0.26978016],\n",
       "       [0.2697384 ],\n",
       "       [0.26975232],\n",
       "       [0.26950595],\n",
       "       [0.26958248],\n",
       "       [0.26941186],\n",
       "       [0.2687788 ],\n",
       "       [0.26962447],\n",
       "       [0.2693716 ],\n",
       "       [0.26933253],\n",
       "       [0.26978493],\n",
       "       [0.26969194],\n",
       "       [0.26969016],\n",
       "       [0.269346  ],\n",
       "       [0.2693888 ],\n",
       "       [0.26938316],\n",
       "       [0.27021393],\n",
       "       [0.26905024],\n",
       "       [0.2693211 ],\n",
       "       [0.26965865],\n",
       "       [0.26957482],\n",
       "       [0.26975337],\n",
       "       [0.26965347],\n",
       "       [0.26964   ],\n",
       "       [0.26961303],\n",
       "       [0.26974198],\n",
       "       [0.26925325],\n",
       "       [0.2693681 ],\n",
       "       [0.26937383],\n",
       "       [0.2695195 ],\n",
       "       [0.26925215],\n",
       "       [0.26997668],\n",
       "       [0.26897496],\n",
       "       [0.26909688],\n",
       "       [0.2699075 ],\n",
       "       [0.26953822],\n",
       "       [0.26942343],\n",
       "       [0.27051798],\n",
       "       [0.2737258 ],\n",
       "       [0.2686942 ],\n",
       "       [0.2696415 ],\n",
       "       [0.26910865],\n",
       "       [0.26978493],\n",
       "       [0.26960006],\n",
       "       [0.26986063],\n",
       "       [0.26908028],\n",
       "       [0.26883915],\n",
       "       [0.26958543],\n",
       "       [0.2694289 ],\n",
       "       [0.2696321 ],\n",
       "       [0.27148557],\n",
       "       [0.26951516],\n",
       "       [0.2697082 ],\n",
       "       [0.26960713],\n",
       "       [0.26959857],\n",
       "       [0.2697934 ],\n",
       "       [0.27000257],\n",
       "       [0.26936185],\n",
       "       [0.26951283],\n",
       "       [0.2696755 ],\n",
       "       [0.26974234],\n",
       "       [0.26977244],\n",
       "       [0.26922202],\n",
       "       [0.26909918],\n",
       "       [0.26983327],\n",
       "       [0.2694518 ],\n",
       "       [0.26983812],\n",
       "       [0.26952025],\n",
       "       [0.26971176],\n",
       "       [0.2696589 ],\n",
       "       [0.26944852],\n",
       "       [0.2698884 ],\n",
       "       [0.26971766],\n",
       "       [0.26946077],\n",
       "       [0.26968518],\n",
       "       [0.26965037],\n",
       "       [0.26856795],\n",
       "       [0.2695727 ],\n",
       "       [0.26909646],\n",
       "       [0.26911545],\n",
       "       [0.26964548],\n",
       "       [0.2694261 ],\n",
       "       [0.26988268],\n",
       "       [0.26897374],\n",
       "       [0.2695032 ],\n",
       "       [0.26928994],\n",
       "       [0.26888594],\n",
       "       [0.2701578 ],\n",
       "       [0.2697943 ],\n",
       "       [0.26969033],\n",
       "       [0.26910347],\n",
       "       [0.26953238],\n",
       "       [0.26909408],\n",
       "       [0.2693731 ],\n",
       "       [0.2695505 ],\n",
       "       [0.96526194],\n",
       "       [0.2696766 ],\n",
       "       [0.26945996],\n",
       "       [0.26947188],\n",
       "       [0.26933822],\n",
       "       [0.26924324],\n",
       "       [0.2698232 ],\n",
       "       [0.26938972],\n",
       "       [0.26922908],\n",
       "       [0.2695817 ],\n",
       "       [0.26924926],\n",
       "       [0.2695245 ],\n",
       "       [0.26956403],\n",
       "       [0.26951128],\n",
       "       [0.2698454 ],\n",
       "       [0.26856795],\n",
       "       [0.26994523],\n",
       "       [0.26954162],\n",
       "       [0.2699792 ],\n",
       "       [0.26913133],\n",
       "       [0.26859784],\n",
       "       [0.2696565 ],\n",
       "       [0.2686183 ],\n",
       "       [0.2696445 ],\n",
       "       [0.26910827],\n",
       "       [0.27013487],\n",
       "       [0.27031624],\n",
       "       [0.2700045 ],\n",
       "       [0.269535  ],\n",
       "       [0.26938385],\n",
       "       [0.26975295],\n",
       "       [0.26917508],\n",
       "       [0.27081633],\n",
       "       [0.26924753],\n",
       "       [0.2695265 ],\n",
       "       [0.2691047 ],\n",
       "       [0.26930118],\n",
       "       [0.26969728],\n",
       "       [0.26963732],\n",
       "       [0.26985133],\n",
       "       [0.26940215],\n",
       "       [0.9620213 ],\n",
       "       [0.26956797],\n",
       "       [0.26927778],\n",
       "       [0.26926735],\n",
       "       [0.2696282 ],\n",
       "       [0.26948038],\n",
       "       [0.26961297],\n",
       "       [0.2696217 ],\n",
       "       [0.26924318],\n",
       "       [0.26998007],\n",
       "       [0.26996928],\n",
       "       [0.2691198 ],\n",
       "       [0.26954612],\n",
       "       [0.26954585],\n",
       "       [0.2695228 ],\n",
       "       [0.26981595],\n",
       "       [0.26918173],\n",
       "       [0.26981837],\n",
       "       [0.26995695],\n",
       "       [0.26970452],\n",
       "       [0.26909438],\n",
       "       [0.2693261 ],\n",
       "       [0.26949236],\n",
       "       [0.9596404 ],\n",
       "       [0.27000672],\n",
       "       [0.26961777],\n",
       "       [0.26935288],\n",
       "       [0.269219  ],\n",
       "       [0.26985863],\n",
       "       [0.2694824 ],\n",
       "       [0.26958025],\n",
       "       [0.26956224],\n",
       "       [0.26927143],\n",
       "       [0.26968354],\n",
       "       [0.26924276],\n",
       "       [0.26949173],\n",
       "       [0.26938328],\n",
       "       [0.26924133],\n",
       "       [0.27014738],\n",
       "       [0.26956344],\n",
       "       [0.27002087],\n",
       "       [0.26959604],\n",
       "       [0.26997414],\n",
       "       [0.2705742 ],\n",
       "       [0.26925698],\n",
       "       [0.26976442],\n",
       "       [0.2694465 ],\n",
       "       [0.26972386],\n",
       "       [0.96496046],\n",
       "       [0.26955533],\n",
       "       [0.26968867],\n",
       "       [0.2700287 ],\n",
       "       [0.26958257],\n",
       "       [0.26926845],\n",
       "       [0.2694048 ],\n",
       "       [0.2698666 ],\n",
       "       [0.27004316],\n",
       "       [0.26992527],\n",
       "       [0.26958764],\n",
       "       [0.26960963],\n",
       "       [0.26969436],\n",
       "       [0.26950458],\n",
       "       [0.26970378],\n",
       "       [0.26954296],\n",
       "       [0.26940832],\n",
       "       [0.27002236],\n",
       "       [0.26956525],\n",
       "       [0.26988378],\n",
       "       [0.26991   ],\n",
       "       [0.2695981 ],\n",
       "       [0.26930064],\n",
       "       [0.26856795],\n",
       "       [0.26992163],\n",
       "       [0.26965502],\n",
       "       [0.26926517],\n",
       "       [0.26948732],\n",
       "       [0.268894  ],\n",
       "       [0.27036715],\n",
       "       [0.26948482],\n",
       "       [0.26964164],\n",
       "       [0.26978713],\n",
       "       [0.5128081 ],\n",
       "       [0.26946458],\n",
       "       [0.27061394],\n",
       "       [0.26955834],\n",
       "       [0.2695506 ],\n",
       "       [0.26922643],\n",
       "       [0.26971775],\n",
       "       [0.26936162],\n",
       "       [0.27044836],\n",
       "       [0.26988882],\n",
       "       [0.26975042],\n",
       "       [0.26931614],\n",
       "       [0.27072895],\n",
       "       [0.26979288],\n",
       "       [0.26930022],\n",
       "       [0.26992986],\n",
       "       [0.2694042 ],\n",
       "       [0.26956716],\n",
       "       [0.26971588],\n",
       "       [0.2694182 ],\n",
       "       [0.26947492],\n",
       "       [0.26983127],\n",
       "       [0.26884404],\n",
       "       [0.26956344],\n",
       "       [0.26919323],\n",
       "       [0.26927063],\n",
       "       [0.26990384],\n",
       "       [0.26963475],\n",
       "       [0.26996303],\n",
       "       [0.26920405],\n",
       "       [0.26983705],\n",
       "       [0.2694728 ],\n",
       "       [0.26937756],\n",
       "       [0.26922008],\n",
       "       [0.27009338],\n",
       "       [0.2696863 ],\n",
       "       [0.9621117 ],\n",
       "       [0.26921535],\n",
       "       [0.26948038],\n",
       "       [0.26996586],\n",
       "       [0.26947495],\n",
       "       [0.26915684],\n",
       "       [0.26991937],\n",
       "       [0.26931283],\n",
       "       [0.26986516],\n",
       "       [0.26963958],\n",
       "       [0.2692016 ],\n",
       "       [0.26944187],\n",
       "       [0.26894432],\n",
       "       [0.2696716 ],\n",
       "       [0.26990086],\n",
       "       [0.96362334],\n",
       "       [0.26857463],\n",
       "       [0.96439856],\n",
       "       [0.27286455],\n",
       "       [0.26925305],\n",
       "       [0.26952523],\n",
       "       [0.26981202],\n",
       "       [0.26960495],\n",
       "       [0.2698949 ],\n",
       "       [0.26961437],\n",
       "       [0.2693715 ],\n",
       "       [0.26931226],\n",
       "       [0.26923776],\n",
       "       [0.2695523 ],\n",
       "       [0.27055347],\n",
       "       [0.27023864],\n",
       "       [0.26944903],\n",
       "       [0.26986203],\n",
       "       [0.26987088],\n",
       "       [0.26983517],\n",
       "       [0.967225  ],\n",
       "       [0.2701201 ],\n",
       "       [0.2692319 ],\n",
       "       [0.9641414 ],\n",
       "       [0.26964995],\n",
       "       [0.26967865],\n",
       "       [0.2694545 ],\n",
       "       [0.26939908],\n",
       "       [0.2700753 ],\n",
       "       [0.26917878],\n",
       "       [0.9621634 ],\n",
       "       [0.2694781 ],\n",
       "       [0.26931337],\n",
       "       [0.26968017],\n",
       "       [0.26947692],\n",
       "       [0.26979128],\n",
       "       [0.26940146],\n",
       "       [0.269919  ],\n",
       "       [0.26991144],\n",
       "       [0.26945445],\n",
       "       [0.2700614 ],\n",
       "       [0.26927748],\n",
       "       [0.26948187],\n",
       "       [0.2692154 ],\n",
       "       [0.26970223],\n",
       "       [0.26987034],\n",
       "       [0.26977098],\n",
       "       [0.26950836],\n",
       "       [0.26927403],\n",
       "       [0.26991642],\n",
       "       [0.26914626],\n",
       "       [0.26976788],\n",
       "       [0.26985374],\n",
       "       [0.26943317],\n",
       "       [0.2691741 ],\n",
       "       [0.26979676],\n",
       "       [0.26909855],\n",
       "       [0.26939845],\n",
       "       [0.2689624 ],\n",
       "       [0.26983222],\n",
       "       [0.2696485 ],\n",
       "       [0.26964512],\n",
       "       [0.96342635],\n",
       "       [0.26979876],\n",
       "       [0.27017418],\n",
       "       [0.26950568],\n",
       "       [0.26941636],\n",
       "       [0.270284  ],\n",
       "       [0.26949376],\n",
       "       [0.27027977],\n",
       "       [0.26887468],\n",
       "       [0.2695917 ],\n",
       "       [0.9488462 ],\n",
       "       [0.27059275],\n",
       "       [0.2699167 ],\n",
       "       [0.26976398],\n",
       "       [0.2696766 ],\n",
       "       [0.2700275 ],\n",
       "       [0.26930276],\n",
       "       [0.26878592],\n",
       "       [0.26967373],\n",
       "       [0.27077448],\n",
       "       [0.26972795],\n",
       "       [0.26965064],\n",
       "       [0.26964486],\n",
       "       [0.26898077],\n",
       "       [0.2692914 ],\n",
       "       [0.26961762],\n",
       "       [0.2694062 ],\n",
       "       [0.26988158],\n",
       "       [0.26944497],\n",
       "       [0.26973847],\n",
       "       [0.26940563],\n",
       "       [0.2696982 ],\n",
       "       [0.26960707],\n",
       "       [0.26926646],\n",
       "       [0.26936698],\n",
       "       [0.26957333],\n",
       "       [0.2699722 ],\n",
       "       [0.26980594],\n",
       "       [0.2695442 ],\n",
       "       [0.26939383],\n",
       "       [0.26936644],\n",
       "       [0.26953962],\n",
       "       [0.26990953],\n",
       "       [0.2694619 ],\n",
       "       [0.9606295 ],\n",
       "       [0.26928997],\n",
       "       [0.26904637],\n",
       "       [0.26971167],\n",
       "       [0.26943094],\n",
       "       [0.2692219 ],\n",
       "       [0.26993132],\n",
       "       [0.27026862],\n",
       "       [0.2698903 ],\n",
       "       [0.26934013],\n",
       "       [0.26949987],\n",
       "       [0.2698637 ],\n",
       "       [0.26936504],\n",
       "       [0.26902717],\n",
       "       [0.26975155],\n",
       "       [0.269032  ],\n",
       "       [0.26891637],\n",
       "       [0.26942915],\n",
       "       [0.2694727 ],\n",
       "       [0.26920623],\n",
       "       [0.2695909 ],\n",
       "       [0.2695937 ],\n",
       "       [0.26952782],\n",
       "       [0.27004182],\n",
       "       [0.26997563],\n",
       "       [0.26975694],\n",
       "       [0.27113035],\n",
       "       [0.26922542],\n",
       "       [0.26934388],\n",
       "       [0.5373469 ],\n",
       "       [0.2700706 ],\n",
       "       [0.26909152],\n",
       "       [0.2691356 ],\n",
       "       [0.26966804],\n",
       "       [0.269561  ],\n",
       "       [0.26978454],\n",
       "       [0.26937425],\n",
       "       [0.9628752 ],\n",
       "       [0.26960292],\n",
       "       [0.26959857],\n",
       "       [0.26933914],\n",
       "       [0.26950678],\n",
       "       [0.26973212],\n",
       "       [0.2695142 ],\n",
       "       [0.26981437],\n",
       "       [0.26897228],\n",
       "       [0.26989192],\n",
       "       [0.2697332 ],\n",
       "       [0.26950714],\n",
       "       [0.26949918],\n",
       "       [0.26856795],\n",
       "       [0.2697617 ],\n",
       "       [0.26972568],\n",
       "       [0.2696112 ],\n",
       "       [0.26868922],\n",
       "       [0.26943675],\n",
       "       [0.26962003],\n",
       "       [0.26966065],\n",
       "       [0.26957804],\n",
       "       [0.26856795],\n",
       "       [0.2698009 ],\n",
       "       [0.2700383 ],\n",
       "       [0.26956692],\n",
       "       [0.2689653 ],\n",
       "       [0.26894847],\n",
       "       [0.26908997],\n",
       "       [0.26972952],\n",
       "       [0.26965058],\n",
       "       [0.26957107],\n",
       "       [0.26997876],\n",
       "       [0.2691246 ],\n",
       "       [0.2697104 ],\n",
       "       [0.2695628 ],\n",
       "       [0.2698956 ],\n",
       "       [0.2691268 ],\n",
       "       [0.2696314 ],\n",
       "       [0.26972514],\n",
       "       [0.26990193],\n",
       "       [0.26946697],\n",
       "       [0.27013883],\n",
       "       [0.2698801 ],\n",
       "       [0.26964495],\n",
       "       [0.26910588],\n",
       "       [0.27022174],\n",
       "       [0.2696702 ],\n",
       "       [0.26976985],\n",
       "       [0.26856795],\n",
       "       [0.2697891 ],\n",
       "       [0.26985115],\n",
       "       [0.26973608],\n",
       "       [0.269601  ],\n",
       "       [0.26961923],\n",
       "       [0.26973617],\n",
       "       [0.2686579 ],\n",
       "       [0.2699837 ],\n",
       "       [0.26990306],\n",
       "       [0.26949757],\n",
       "       [0.26998526],\n",
       "       [0.26892203],\n",
       "       [0.26978603],\n",
       "       [0.2694538 ],\n",
       "       [0.26986966],\n",
       "       [0.26992697],\n",
       "       [0.2697576 ],\n",
       "       [0.26973474],\n",
       "       [0.2694185 ],\n",
       "       [0.9583294 ],\n",
       "       [0.26937065],\n",
       "       [0.26919305],\n",
       "       [0.26938176],\n",
       "       [0.96401864],\n",
       "       [0.2693761 ],\n",
       "       [0.26929852],\n",
       "       [0.26968092],\n",
       "       [0.26916695],\n",
       "       [0.26924062],\n",
       "       [0.2697941 ],\n",
       "       [0.26934358],\n",
       "       [0.269687  ],\n",
       "       [0.26986307],\n",
       "       [0.26910788],\n",
       "       [0.26942307],\n",
       "       [0.26961017],\n",
       "       [0.27020904],\n",
       "       [0.91743135],\n",
       "       [0.26966363],\n",
       "       [0.26945275],\n",
       "       [0.26856795],\n",
       "       [0.26967207],\n",
       "       [0.2692341 ],\n",
       "       [0.2693444 ],\n",
       "       [0.26969987],\n",
       "       [0.26953858],\n",
       "       [0.26940084],\n",
       "       [0.26935527],\n",
       "       [0.2692216 ],\n",
       "       [0.26918206],\n",
       "       [0.27006385],\n",
       "       [0.26963678],\n",
       "       [0.26892897],\n",
       "       [0.26974475],\n",
       "       [0.26960123],\n",
       "       [0.26949698],\n",
       "       [0.26977268],\n",
       "       [0.26944786],\n",
       "       [0.26992226],\n",
       "       [0.26960275],\n",
       "       [0.269956  ],\n",
       "       [0.26952973],\n",
       "       [0.2698175 ],\n",
       "       [0.26936445],\n",
       "       [0.2697913 ],\n",
       "       [0.26902542],\n",
       "       [0.26925725],\n",
       "       [0.26963225],\n",
       "       [0.26953793],\n",
       "       [0.26986006],\n",
       "       [0.26956844],\n",
       "       [0.27099377],\n",
       "       [0.269627  ],\n",
       "       [0.96294093],\n",
       "       [0.2690027 ],\n",
       "       [0.26915964],\n",
       "       [0.2692291 ],\n",
       "       [0.26926613],\n",
       "       [0.26988912],\n",
       "       [0.27038044],\n",
       "       [0.9615556 ],\n",
       "       [0.2701307 ],\n",
       "       [0.2710318 ],\n",
       "       [0.26987815],\n",
       "       [0.26856795],\n",
       "       [0.26985702],\n",
       "       [0.2698197 ],\n",
       "       [0.2694426 ],\n",
       "       [0.26946592],\n",
       "       [0.269426  ],\n",
       "       [0.2694795 ],\n",
       "       [0.26985642],\n",
       "       [0.2696896 ],\n",
       "       [0.26946464],\n",
       "       [0.26967844],\n",
       "       [0.26955673],\n",
       "       [0.26951033],\n",
       "       [0.269646  ],\n",
       "       [0.26981965],\n",
       "       [0.26902547],\n",
       "       [0.9568696 ],\n",
       "       [0.26977944],\n",
       "       [0.2692525 ],\n",
       "       [0.26934978],\n",
       "       [0.26976573],\n",
       "       [0.26944625],\n",
       "       [0.26920798],\n",
       "       [0.26982898],\n",
       "       [0.26956287],\n",
       "       [0.26950145],\n",
       "       [0.26969272],\n",
       "       [0.27012944],\n",
       "       [0.26934168],\n",
       "       [0.2690505 ],\n",
       "       [0.26951152],\n",
       "       [0.26998568],\n",
       "       [0.27218977],\n",
       "       [0.26921645],\n",
       "       [0.2692224 ],\n",
       "       [0.2694765 ],\n",
       "       [0.2699261 ],\n",
       "       [0.26957726],\n",
       "       [0.27000228],\n",
       "       [0.2688454 ],\n",
       "       [0.2714608 ],\n",
       "       [0.27073255],\n",
       "       [0.26978967],\n",
       "       [0.2696209 ],\n",
       "       [0.26871616],\n",
       "       [0.26913962],\n",
       "       [0.26953027],\n",
       "       [0.26856795],\n",
       "       [0.2702411 ],\n",
       "       [0.2699127 ],\n",
       "       [0.2690501 ],\n",
       "       [0.26954612],\n",
       "       [0.26944253],\n",
       "       [0.27005294],\n",
       "       [0.26974052],\n",
       "       [0.26986918],\n",
       "       [0.26875648],\n",
       "       [0.26975873],\n",
       "       [0.27163875],\n",
       "       [0.2694604 ],\n",
       "       [0.26955423],\n",
       "       [0.269326  ],\n",
       "       [0.26912612],\n",
       "       [0.2690129 ],\n",
       "       [0.26910725],\n",
       "       [0.2693305 ],\n",
       "       [0.26953873],\n",
       "       [0.26976216],\n",
       "       [0.269593  ],\n",
       "       [0.2699017 ],\n",
       "       [0.26981527],\n",
       "       [0.26866874],\n",
       "       [0.26957014],\n",
       "       [0.26942116],\n",
       "       [0.2697953 ],\n",
       "       [0.26942626],\n",
       "       [0.26972213],\n",
       "       [0.2691824 ],\n",
       "       [0.2689269 ],\n",
       "       [0.2691548 ],\n",
       "       [0.26987112],\n",
       "       [0.26945043],\n",
       "       [0.2694222 ],\n",
       "       [0.26973513],\n",
       "       [0.26899704],\n",
       "       [0.27007884],\n",
       "       [0.2698466 ],\n",
       "       [0.26965418],\n",
       "       [0.26860276],\n",
       "       [0.26931915],\n",
       "       [0.2692337 ],\n",
       "       [0.26910073],\n",
       "       [0.26948234],\n",
       "       [0.2694296 ],\n",
       "       [0.278879  ],\n",
       "       [0.26884386],\n",
       "       [0.26969418],\n",
       "       [0.2691245 ],\n",
       "       [0.2692548 ],\n",
       "       [0.270003  ],\n",
       "       [0.2696606 ],\n",
       "       [0.27005118],\n",
       "       [0.26976725],\n",
       "       [0.269537  ],\n",
       "       [0.2695882 ],\n",
       "       [0.2696909 ],\n",
       "       [0.26917192],\n",
       "       [0.2690995 ],\n",
       "       [0.26932323],\n",
       "       [0.26923102],\n",
       "       [0.26958257],\n",
       "       [0.2700282 ],\n",
       "       [0.9546526 ],\n",
       "       [0.27006614],\n",
       "       [0.2695879 ],\n",
       "       [0.26964688],\n",
       "       [0.2697695 ],\n",
       "       [0.26917627],\n",
       "       [0.2698538 ],\n",
       "       [0.2695976 ],\n",
       "       [0.26997247],\n",
       "       [0.26976746],\n",
       "       [0.26965806],\n",
       "       [0.26925686],\n",
       "       [0.26944146],\n",
       "       [0.26939774],\n",
       "       [0.26981023],\n",
       "       [0.2691213 ],\n",
       "       [0.26906276],\n",
       "       [0.2696744 ],\n",
       "       [0.2698605 ],\n",
       "       [0.27211112],\n",
       "       [0.26961663],\n",
       "       [0.26948154],\n",
       "       [0.27002564],\n",
       "       [0.2692908 ],\n",
       "       [0.26882863],\n",
       "       [0.26895064],\n",
       "       [0.2686863 ],\n",
       "       [0.2694394 ],\n",
       "       [0.26937762],\n",
       "       [0.26988503],\n",
       "       [0.26946843],\n",
       "       [0.26979145],\n",
       "       [0.26954398],\n",
       "       [0.26952797],\n",
       "       [0.2695835 ],\n",
       "       [0.26908585],\n",
       "       [0.26974553],\n",
       "       [0.26982933],\n",
       "       [0.26934552],\n",
       "       [0.2694791 ],\n",
       "       [0.26972222],\n",
       "       [0.26970485],\n",
       "       [0.27017647],\n",
       "       [0.270019  ],\n",
       "       [0.2703798 ],\n",
       "       [0.2693952 ],\n",
       "       [0.269864  ],\n",
       "       [0.26944175],\n",
       "       [0.26959804],\n",
       "       [0.26965597],\n",
       "       [0.26991898],\n",
       "       [0.26927412],\n",
       "       [0.26971886],\n",
       "       [0.27033654],\n",
       "       [0.2696433 ],\n",
       "       [0.26935697],\n",
       "       [0.9652115 ],\n",
       "       [0.26986426],\n",
       "       [0.26940852],\n",
       "       [0.2696352 ],\n",
       "       [0.26998332],\n",
       "       [0.26952693],\n",
       "       [0.2700915 ],\n",
       "       [0.26952022],\n",
       "       [0.26983032],\n",
       "       [0.26937297],\n",
       "       [0.26901978],\n",
       "       [0.26977843],\n",
       "       [0.26925442],\n",
       "       [0.26965445],\n",
       "       [0.2694785 ],\n",
       "       [0.2694515 ],\n",
       "       [0.26941696],\n",
       "       [0.26959217],\n",
       "       [0.26960537],\n",
       "       [0.26984876],\n",
       "       [0.26942834],\n",
       "       [0.26996818],\n",
       "       [0.26964372],\n",
       "       [0.9631071 ],\n",
       "       [0.26969546],\n",
       "       [0.26973975],\n",
       "       [0.26984605],\n",
       "       [0.26899773],\n",
       "       [0.2696227 ],\n",
       "       [0.2708394 ],\n",
       "       [0.269912  ],\n",
       "       [0.26973295],\n",
       "       [0.26940697],\n",
       "       [0.269414  ],\n",
       "       [0.2695466 ],\n",
       "       [0.2696782 ],\n",
       "       [0.26919463],\n",
       "       [0.2696509 ],\n",
       "       [0.26988125],\n",
       "       [0.2695715 ],\n",
       "       [0.27002472],\n",
       "       [0.26945606],\n",
       "       [0.26952255],\n",
       "       [0.26856795],\n",
       "       [0.26875082],\n",
       "       [0.2699725 ],\n",
       "       [0.26924816],\n",
       "       [0.27030188],\n",
       "       [0.2699199 ],\n",
       "       [0.26949298],\n",
       "       [0.26952472],\n",
       "       [0.26962706],\n",
       "       [0.26932475],\n",
       "       [0.27012607],\n",
       "       [0.26999757]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask], ) #output represents likelihood example was in the positive class\n",
    "#these are all about the same and not very confident either way about whether example is in the class or not\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26856795, 0.26857463, 0.26859784, 0.26860276, 0.2686183 ,\n",
       "       0.2686579 , 0.26866874, 0.2686863 , 0.26868922, 0.2686942 ,\n",
       "       0.26871616, 0.26875082, 0.26875648, 0.2687788 , 0.2687808 ,\n",
       "       0.2687848 , 0.26878592, 0.26882863, 0.26882908, 0.26883084,\n",
       "       0.26883557, 0.26883915, 0.26884386, 0.26884404, 0.2688454 ,\n",
       "       0.26887468, 0.26887497, 0.26888594, 0.26888964, 0.26889256,\n",
       "       0.268894  , 0.26891637, 0.26892203, 0.2689269 , 0.26892897,\n",
       "       0.26894432, 0.26894847, 0.26895064, 0.2689624 , 0.2689653 ,\n",
       "       0.26896748, 0.26897228, 0.26897374, 0.26897496, 0.26898077,\n",
       "       0.26899704, 0.26899773, 0.268998  , 0.2690027 , 0.2690129 ,\n",
       "       0.26901948, 0.26901978, 0.26902542, 0.26902547, 0.26902717,\n",
       "       0.269032  , 0.26903936, 0.26904637, 0.26904702, 0.2690501 ,\n",
       "       0.26905024, 0.2690505 , 0.26906276, 0.26906562, 0.26908028,\n",
       "       0.26908585, 0.26908997, 0.26909152, 0.26909408, 0.26909438,\n",
       "       0.26909646, 0.26909688, 0.26909855, 0.26909918, 0.2690995 ,\n",
       "       0.26910073, 0.26910347, 0.2691047 , 0.26910588, 0.26910725,\n",
       "       0.26910788, 0.26910827, 0.26910865, 0.26911068, 0.26911524,\n",
       "       0.26911545, 0.2691198 , 0.2691213 , 0.2691245 , 0.2691246 ,\n",
       "       0.26912612, 0.2691268 , 0.26912746, 0.26913133, 0.2691356 ,\n",
       "       0.26913646, 0.26913962, 0.26914328, 0.26914626, 0.2691548 ,\n",
       "       0.2691555 , 0.26915684, 0.2691587 , 0.26915964, 0.26916695,\n",
       "       0.26917192, 0.2691741 , 0.26917508, 0.26917627, 0.26917878,\n",
       "       0.26918173, 0.26918206, 0.2691821 , 0.2691824 , 0.2691869 ,\n",
       "       0.26919058, 0.26919305, 0.26919323, 0.26919463, 0.26919886,\n",
       "       0.2692016 , 0.26920405, 0.26920575, 0.26920623, 0.26920798,\n",
       "       0.26920846, 0.26921535, 0.2692154 , 0.26921645, 0.269219  ,\n",
       "       0.26922008, 0.2692216 , 0.2692219 , 0.26922202, 0.2692224 ,\n",
       "       0.26922375, 0.26922542, 0.2692264 , 0.26922643, 0.26922768,\n",
       "       0.26922908, 0.2692291 , 0.26923102, 0.2692319 , 0.26923263,\n",
       "       0.2692337 , 0.2692341 , 0.26923776, 0.26923922, 0.26924035,\n",
       "       0.26924062, 0.26924133, 0.2692421 , 0.26924276, 0.26924318,\n",
       "       0.26924324, 0.26924753, 0.26924816, 0.26924857, 0.26924884,\n",
       "       0.26924926, 0.26925215, 0.2692525 , 0.26925305, 0.26925325,\n",
       "       0.26925442, 0.2692548 , 0.26925686, 0.26925698, 0.26925725,\n",
       "       0.26926517, 0.26926613, 0.26926646, 0.26926735, 0.26926756,\n",
       "       0.26926845, 0.26927063, 0.26927143, 0.26927403, 0.26927412,\n",
       "       0.2692753 , 0.26927724, 0.26927748, 0.26927778, 0.26928994,\n",
       "       0.26928997, 0.2692908 , 0.2692914 , 0.26929158, 0.26929736,\n",
       "       0.26929784, 0.26929852, 0.26930022, 0.26930064, 0.26930118,\n",
       "       0.26930276, 0.26930496, 0.26931226, 0.26931283, 0.26931337,\n",
       "       0.26931408, 0.26931614, 0.26931915, 0.26932028, 0.2693211 ,\n",
       "       0.269322  , 0.26932323, 0.26932475, 0.269326  , 0.2693261 ,\n",
       "       0.26932856, 0.2693305 , 0.26933178, 0.26933253, 0.26933822,\n",
       "       0.26933914, 0.26934013, 0.26934168, 0.26934358, 0.26934388,\n",
       "       0.2693444 , 0.2693451 , 0.26934552, 0.269346  , 0.26934978,\n",
       "       0.26935288, 0.26935527, 0.26935697, 0.26936162, 0.26936185,\n",
       "       0.26936445, 0.26936504, 0.269366  , 0.26936644, 0.26936698,\n",
       "       0.2693681 , 0.26937065, 0.26937124, 0.2693715 , 0.2693716 ,\n",
       "       0.26937297, 0.2693731 , 0.26937383, 0.26937425, 0.2693747 ,\n",
       "       0.269376  , 0.2693761 , 0.26937643, 0.26937756, 0.26937762,\n",
       "       0.26938176, 0.2693822 , 0.2693827 , 0.26938304, 0.26938316,\n",
       "       0.26938328, 0.26938358, 0.26938385, 0.26938805, 0.26938874,\n",
       "       0.2693888 , 0.26938972, 0.26939383, 0.2693952 , 0.26939774,\n",
       "       0.2693981 , 0.26939845, 0.26939908, 0.26940084, 0.26940146,\n",
       "       0.26940215, 0.26940218, 0.26940274, 0.2694042 , 0.2694048 ,\n",
       "       0.26940563, 0.26940593, 0.2694062 , 0.26940697, 0.26940832,\n",
       "       0.26940852, 0.26941186, 0.26941344, 0.269414  , 0.2694149 ,\n",
       "       0.26941636, 0.26941696, 0.2694182 , 0.2694185 , 0.2694187 ,\n",
       "       0.26942116, 0.2694222 , 0.26942307, 0.26942343, 0.269426  ,\n",
       "       0.2694261 , 0.26942626, 0.26942834, 0.2694289 , 0.26942915,\n",
       "       0.2694296 , 0.26942965, 0.26943094, 0.26943278, 0.26943317,\n",
       "       0.26943675, 0.26943874, 0.2694394 , 0.26944146, 0.26944175,\n",
       "       0.26944187, 0.26944253, 0.2694426 , 0.26944476, 0.26944497,\n",
       "       0.26944625, 0.2694465 , 0.26944786, 0.26944852, 0.26944903,\n",
       "       0.26945043, 0.26945108, 0.2694515 , 0.2694518 , 0.26945275,\n",
       "       0.2694538 , 0.26945445, 0.2694545 , 0.26945606, 0.2694566 ,\n",
       "       0.26945677, 0.26945996, 0.2694604 , 0.26946077, 0.26946163,\n",
       "       0.2694619 , 0.26946458, 0.26946464, 0.26946592, 0.26946697,\n",
       "       0.26946843, 0.26946864, 0.2694718 , 0.26947188, 0.26947194,\n",
       "       0.26947248, 0.2694727 , 0.2694728 , 0.26947492, 0.26947495,\n",
       "       0.2694765 , 0.26947692, 0.2694781 , 0.2694785 , 0.2694791 ,\n",
       "       0.2694795 , 0.26948008, 0.26948038, 0.26948148, 0.26948154,\n",
       "       0.26948187, 0.26948234, 0.2694824 , 0.26948482, 0.2694856 ,\n",
       "       0.26948634, 0.26948732, 0.26949173, 0.26949218, 0.26949236,\n",
       "       0.26949272, 0.26949298, 0.2694936 , 0.26949376, 0.26949388,\n",
       "       0.26949593, 0.26949698, 0.26949757, 0.26949918, 0.26949987,\n",
       "       0.26950145, 0.2695032 , 0.26950458, 0.26950568, 0.26950595,\n",
       "       0.26950678, 0.26950714, 0.26950788, 0.2695081 , 0.26950836,\n",
       "       0.26951033, 0.26951128, 0.26951152, 0.26951283, 0.2695142 ,\n",
       "       0.26951507, 0.26951516, 0.26951787, 0.26951844, 0.2695195 ,\n",
       "       0.26951978, 0.26952022, 0.26952025, 0.26952112, 0.26952195,\n",
       "       0.26952255, 0.2695228 , 0.2695245 , 0.26952472, 0.26952523,\n",
       "       0.2695263 , 0.2695265 , 0.26952693, 0.26952782, 0.26952797,\n",
       "       0.26952973, 0.26952976, 0.26953027, 0.26953238, 0.269535  ,\n",
       "       0.269537  , 0.26953793, 0.26953822, 0.26953858, 0.26953873,\n",
       "       0.26953962, 0.2695399 , 0.26954162, 0.26954296, 0.26954398,\n",
       "       0.2695442 , 0.269545  , 0.26954585, 0.26954612, 0.2695466 ,\n",
       "       0.2695505 , 0.2695506 , 0.2695523 , 0.26955423, 0.26955533,\n",
       "       0.2695566 , 0.26955673, 0.26955834, 0.26955855, 0.269561  ,\n",
       "       0.26956224, 0.2695628 , 0.26956287, 0.26956344, 0.26956403,\n",
       "       0.2695646 , 0.26956525, 0.26956692, 0.26956716, 0.2695677 ,\n",
       "       0.26956797, 0.26956844, 0.2695693 , 0.26957014, 0.26957107,\n",
       "       0.2695715 , 0.2695727 , 0.26957333, 0.26957482, 0.26957658,\n",
       "       0.26957726, 0.26957804, 0.26957875, 0.26958004, 0.26958025,\n",
       "       0.2695817 , 0.26958188, 0.26958248, 0.26958257, 0.2695835 ,\n",
       "       0.26958543, 0.26958764, 0.2695879 , 0.2695882 , 0.26959   ,\n",
       "       0.2695909 , 0.2695917 , 0.26959208, 0.26959217, 0.269593  ,\n",
       "       0.2695937 , 0.26959413, 0.26959604, 0.2695976 , 0.26959804,\n",
       "       0.2695981 , 0.26959857, 0.26959875, 0.26959947, 0.26960006,\n",
       "       0.269601  , 0.26960123, 0.26960275, 0.26960292, 0.26960468,\n",
       "       0.26960495, 0.26960537, 0.26960644, 0.26960707, 0.26960713,\n",
       "       0.26960963, 0.26961017, 0.26961026, 0.2696112 , 0.26961234,\n",
       "       0.26961297, 0.26961303, 0.26961437, 0.2696144 , 0.2696158 ,\n",
       "       0.26961663, 0.26961762, 0.26961777, 0.26961792, 0.26961914,\n",
       "       0.26961923, 0.26962003, 0.2696209 , 0.2696217 , 0.2696227 ,\n",
       "       0.26962447, 0.269627  , 0.26962706, 0.26962775, 0.2696282 ,\n",
       "       0.26962993, 0.2696303 , 0.26963043, 0.2696312 , 0.2696314 ,\n",
       "       0.2696321 , 0.26963225, 0.26963404, 0.26963475, 0.2696352 ,\n",
       "       0.26963606, 0.26963678, 0.26963732, 0.26963776, 0.26963958,\n",
       "       0.26964   , 0.26964018, 0.26964098, 0.2696415 , 0.26964164,\n",
       "       0.2696433 , 0.26964372, 0.2696445 , 0.26964486, 0.26964495,\n",
       "       0.26964512, 0.26964545, 0.26964548, 0.269646  , 0.26964688,\n",
       "       0.2696485 , 0.26964995, 0.2696501 , 0.26965037, 0.26965058,\n",
       "       0.26965064, 0.2696509 , 0.26965114, 0.26965347, 0.26965418,\n",
       "       0.26965445, 0.26965502, 0.26965597, 0.2696564 , 0.2696565 ,\n",
       "       0.2696573 , 0.26965806, 0.26965865, 0.2696589 , 0.2696606 ,\n",
       "       0.26966065, 0.26966098, 0.26966363, 0.26966494, 0.26966804,\n",
       "       0.2696685 , 0.26966882, 0.2696702 , 0.2696716 , 0.26967207,\n",
       "       0.26967373, 0.2696744 , 0.2696755 , 0.2696766 , 0.26967734,\n",
       "       0.2696782 , 0.26967844, 0.26967865, 0.26968017, 0.26968092,\n",
       "       0.26968217, 0.26968333, 0.2696834 , 0.26968354, 0.26968518,\n",
       "       0.2696863 , 0.269687  , 0.26968867, 0.2696888 , 0.2696896 ,\n",
       "       0.26968986, 0.26969016, 0.26969033, 0.2696909 , 0.26969194,\n",
       "       0.26969272, 0.26969418, 0.26969436, 0.26969546, 0.26969713,\n",
       "       0.26969728, 0.2696982 , 0.26969987, 0.26970223, 0.26970378,\n",
       "       0.26970384, 0.26970452, 0.26970485, 0.26970747, 0.2697082 ,\n",
       "       0.26970997, 0.2697104 , 0.26971167, 0.26971176, 0.2697132 ,\n",
       "       0.26971588, 0.26971766, 0.26971775, 0.26971886, 0.26972002,\n",
       "       0.2697206 , 0.26972213, 0.26972222, 0.26972386, 0.26972514,\n",
       "       0.26972568, 0.26972795, 0.26972952, 0.26972982, 0.26973036,\n",
       "       0.2697318 , 0.26973212, 0.26973295, 0.2697332 , 0.26973474,\n",
       "       0.26973513, 0.26973608, 0.26973617, 0.2697384 , 0.26973847,\n",
       "       0.26973975, 0.26974052, 0.26974198, 0.26974234, 0.26974475,\n",
       "       0.26974553, 0.26974756, 0.26975042, 0.26975155, 0.26975232,\n",
       "       0.26975295, 0.26975337, 0.26975536, 0.26975694, 0.2697576 ,\n",
       "       0.26975873, 0.2697608 , 0.2697617 , 0.26976216, 0.26976398,\n",
       "       0.26976442, 0.26976573, 0.26976612, 0.26976725, 0.26976746,\n",
       "       0.26976788, 0.2697695 , 0.26976985, 0.26977098, 0.26977244,\n",
       "       0.26977268, 0.26977843, 0.26977944, 0.26978016, 0.26978347,\n",
       "       0.26978454, 0.26978493, 0.26978603, 0.26978713, 0.2697891 ,\n",
       "       0.26978967, 0.26979128, 0.2697913 , 0.26979145, 0.26979288,\n",
       "       0.2697934 , 0.2697941 , 0.2697943 , 0.2697953 , 0.26979676,\n",
       "       0.26979876, 0.2698009 , 0.2698029 , 0.26980594, 0.26980665,\n",
       "       0.26981023, 0.26981202, 0.26981437, 0.26981527, 0.26981595,\n",
       "       0.2698175 , 0.26981837, 0.26981965, 0.2698197 , 0.2698232 ,\n",
       "       0.26982442, 0.26982898, 0.26982933, 0.26983032, 0.26983127,\n",
       "       0.26983222, 0.26983327, 0.26983517, 0.26983705, 0.26983812,\n",
       "       0.2698454 , 0.26984605, 0.2698466 , 0.26984876, 0.26984984,\n",
       "       0.26985115, 0.26985124, 0.26985133, 0.26985374, 0.2698538 ,\n",
       "       0.26985478, 0.26985642, 0.26985702, 0.26985863, 0.26986006,\n",
       "       0.2698605 , 0.26986063, 0.26986203, 0.26986307, 0.2698637 ,\n",
       "       0.269864  , 0.26986426, 0.26986516, 0.2698666 , 0.26986918,\n",
       "       0.26986966, 0.26987034, 0.26987088, 0.26987112, 0.26987273,\n",
       "       0.269874  , 0.26987815, 0.2698801 , 0.26988125, 0.26988158,\n",
       "       0.26988268, 0.26988378, 0.2698845 , 0.26988456, 0.26988503,\n",
       "       0.2698884 , 0.26988882, 0.26988912, 0.2698903 , 0.26989192,\n",
       "       0.26989442, 0.2698949 , 0.2698956 , 0.26989737, 0.26990086,\n",
       "       0.2699017 , 0.26990193, 0.26990306, 0.26990384, 0.2699075 ,\n",
       "       0.269908  , 0.26990953, 0.26991   , 0.26991144, 0.269912  ,\n",
       "       0.2699127 , 0.26991567, 0.26991642, 0.2699167 , 0.26991898,\n",
       "       0.269919  , 0.26991937, 0.2699199 , 0.26992163, 0.26992226,\n",
       "       0.26992527, 0.2699261 , 0.26992697, 0.26992986, 0.26993132,\n",
       "       0.26994434, 0.26994514, 0.26994523, 0.2699521 , 0.269956  ,\n",
       "       0.26995695, 0.26996303, 0.26996586, 0.26996818, 0.26996928,\n",
       "       0.2699722 , 0.26997247, 0.2699725 , 0.26997414, 0.26997563,\n",
       "       0.26997668, 0.26997846, 0.26997876, 0.2699792 , 0.26998007,\n",
       "       0.26998332, 0.2699837 , 0.2699849 , 0.26998526, 0.26998568,\n",
       "       0.26998687, 0.2699913 , 0.26999757, 0.27000228, 0.27000257,\n",
       "       0.270003  , 0.2700045 , 0.27000672, 0.27001   , 0.2700152 ,\n",
       "       0.270019  , 0.27002034, 0.27002087, 0.27002236, 0.27002472,\n",
       "       0.27002564, 0.2700275 , 0.2700282 , 0.2700287 , 0.2700383 ,\n",
       "       0.27004182, 0.27004316, 0.27005118, 0.27005294, 0.2700614 ,\n",
       "       0.27006373, 0.27006385, 0.27006558, 0.27006614, 0.27006847,\n",
       "       0.27006915, 0.2700706 , 0.2700753 , 0.27007884, 0.27008647,\n",
       "       0.2700915 , 0.27009338, 0.2700995 , 0.2701201 , 0.2701217 ,\n",
       "       0.27012607, 0.27012944, 0.2701307 , 0.27013487, 0.27013883,\n",
       "       0.27014738, 0.27015543, 0.27015653, 0.2701578 , 0.27016804,\n",
       "       0.27017418, 0.27017647, 0.2702018 , 0.27020904, 0.270213  ,\n",
       "       0.27021393, 0.27022174, 0.27023864, 0.2702411 , 0.27025193,\n",
       "       0.2702536 , 0.27026862, 0.27027977, 0.270284  , 0.27030188,\n",
       "       0.27031472, 0.27031624, 0.27033654, 0.27034485, 0.27036715,\n",
       "       0.2703798 , 0.27038044, 0.2704292 , 0.27044836, 0.27051798,\n",
       "       0.27054313, 0.27055347, 0.2705742 , 0.27059275, 0.27059484,\n",
       "       0.27061394, 0.27065745, 0.27072895, 0.27073255, 0.27077448,\n",
       "       0.27081633, 0.2708394 , 0.270849  , 0.27099377, 0.2710021 ,\n",
       "       0.2710318 , 0.27113035, 0.2714608 , 0.27148557, 0.27163875,\n",
       "       0.27211112, 0.27218977, 0.27281344, 0.27286455, 0.2737258 ,\n",
       "       0.2745173 , 0.278879  , 0.5128081 , 0.5373469 , 0.91743135,\n",
       "       0.9488462 , 0.9546526 , 0.9554304 , 0.9568696 , 0.9583294 ,\n",
       "       0.9596404 , 0.9606295 , 0.9607397 , 0.9615556 , 0.9620213 ,\n",
       "       0.9621117 , 0.9621634 , 0.9623409 , 0.9628752 , 0.96294093,\n",
       "       0.96303445, 0.9631071 , 0.96342635, 0.96354455, 0.96362334,\n",
       "       0.9636564 , 0.96401864, 0.9641414 , 0.96425927, 0.9643061 ,\n",
       "       0.96439856, 0.96471447, 0.96496046, 0.9652115 , 0.96526194,\n",
       "       0.9656086 , 0.967225  ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(predictions) #At least the English model is learning ... Could look at cases where English is failing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6930848956108093 / Test accuracy: 0.5206666588783264\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-7565c2f4f57a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_label\"] = np.nan\n",
      "<ipython-input-22-7565c2f4f57a>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  amharic_train[\"predicted_stat\"] = predictions\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n",
      "/home/joanieweaver/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "#Creating some new columns & printing out a csv with the predicted labels\n",
    "amharic_train[\"predicted_label\"] = np.nan\n",
    "amharic_train[\"predicted_stat\"] = predictions\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] >= 0.5), \"predicted_label\"] = 1\n",
    "amharic_train.loc[(amharic_train[\"predicted_stat\"] < 0.5), \"predicted_label\"] = 0\n",
    "amharic_train.to_csv(\"Amharic_train_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2: Following Model 2 from BERT Walkthrough notebook\n",
    "Updating learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2700/2700 [==============================] - 413s 150ms/step - loss: 0.6996 - accuracy: 0.4995 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 2/5\n",
      "2700/2700 [==============================] - 404s 150ms/step - loss: 0.6945 - accuracy: 0.4970 - val_loss: 0.6933 - val_accuracy: 0.5040\n",
      "Epoch 3/5\n",
      "2700/2700 [==============================] - 404s 150ms/step - loss: 0.6941 - accuracy: 0.4976 - val_loss: 0.6933 - val_accuracy: 0.5040\n",
      "Epoch 4/5\n",
      "1981/2700 [=====================>........] - ETA: 1:46 - loss: 0.6938 - accuracy: 0.5006"
     ]
    }
   ],
   "source": [
    "#do same thing as above but change learning rate in Adam below, need to get fresh bert model\n",
    "try:\n",
    "    del classification_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    del bert_model\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "#NEED to import and load both of these\n",
    "#using the pretrained model called bert-base-cased\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "classification_model = create_classification_model(optimizer=tf.keras.optimizers.Adam(0.00005))\n",
    "\n",
    "classification_model.fit([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask],\n",
    "                         y_train,\n",
    "                         validation_data=([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev),\n",
    "                        epochs=5,\n",
    "                        batch_size=10)\n",
    "\n",
    "classification_model.predict([x_train.input_ids, x_train.token_type_ids, x_train.attention_mask])\n",
    "\n",
    "\n",
    "#This looks a little worse, not sure why it's now predicting 54% consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6977327466011047 / Test accuracy: 0.4959999918937683\n"
     ]
    }
   ],
   "source": [
    "# Generate generalization metrics\n",
    "score = classification_model.evaluate([x_dev.input_ids, x_dev.token_type_ids, x_dev.attention_mask],\n",
    "                         y_dev, verbose=0)\n",
    "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attention_mask_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_ids_layer (InputLayer)    [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_type_ids_layer (InputLaye [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 108310272   attention_mask_layer[0][0]       \n",
      "                                                                 input_ids_layer[0][0]            \n",
      "                                                                 token_type_ids_layer[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "get_first_vector (Lambda)       (None, 768)          0           tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "hidden_layer (Dense)            (None, 200)          153800      get_first_vector[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "classification_layer (Dense)    (None, 1)            201         hidden_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 108,464,273\n",
      "Trainable params: 108,464,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8b12a87bde52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassification_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/BERT_multilingual_adam_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/BERT_multilingual_v1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'save_model'"
     ]
    }
   ],
   "source": [
    "classification_model.save_model(\"models/BERT_multilingual_adam_v1\")\n",
    "tokenizer.save_pretrained(\"tokenizers/BERT_multilingual_v1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
